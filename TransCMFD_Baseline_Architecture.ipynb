{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOA/5ckOtXjgQS9vvRx0CUu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitajain523/TransCMFD_Copy_Move_Forgery_Detection/blob/main/TransCMFD_Baseline_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n",
        "#mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "m7lc2v1yW1OG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab5936f-2849-4ec3-e23e-98cd4c952d86"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA device name: NVIDIA A100-SXM4-40GB\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#  Helper Modules\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
        "        self.gn = nn.GroupNorm(32, out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.with_nonlinearity = with_nonlinearity\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.gn(x)\n",
        "        if self.with_nonlinearity:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "class Bridge(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.bridge = nn.Sequential(\n",
        "            ConvBlock(in_channels, out_channels),\n",
        "            ConvBlock(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bridge(x)\n",
        "\n",
        "# ----------------------------------------------------------------------------- #\n",
        "#upblock\n",
        "class UpBlockForUNetWithResNet50(nn.Module):\n",
        "    #Consists of Upsample ->(Concatenation with skip connection)->ConvBlock.\n",
        "    def __init__(self, in_channels_after_concat, out_channels, up_conv_in_channels, up_conv_out_channels,\n",
        "                 upsampling_method=\"bilinear\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsampling_method = upsampling_method\n",
        "        if upsampling_method == \"conv_transpose\":\n",
        "            self.upsample_layer = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
        "        elif upsampling_method == \"bilinear\":\n",
        "            self.upsample_layer = nn.Upsample(scale_factor=2.0, mode='bilinear', align_corners=False)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported upsampling_method\")\n",
        "\n",
        "        self.conv_block = ConvBlock(in_channels_after_concat, out_channels)\n",
        "\n",
        "    def forward(self, up_x, down_x):\n",
        "        up_x = self.upsample_layer(up_x)\n",
        "\n",
        "        if up_x.shape[2] != down_x.shape[2] or up_x.shape[3] != down_x.shape[3]:\n",
        "            up_x = F.interpolate(up_x, size=(down_x.shape[2], down_x.shape[3]), mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = torch.cat([up_x, down_x], 1)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------- #\n",
        "\n",
        "#CNN Encoder\n",
        "class Encoder(nn.Module):\n",
        "    #CNN Encoder based on a pre-trained ResNet-50, using the channels for resnet 50 itself.\n",
        "    DEPTH = 6\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        resnet = torchvision.models.resnet.resnet50(pretrained=True)##using pretrained weights\n",
        "\n",
        "        self.input_block = nn.Sequential(*list(resnet.children()))[:4]\n",
        "\n",
        "        down_blocks = []\n",
        "        for bottleneck_stage in list(resnet.children())[4:]:\n",
        "            if isinstance(bottleneck_stage, nn.Sequential):\n",
        "                down_blocks.append(bottleneck_stage)\n",
        "        self.down_blocks = nn.ModuleList(down_blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre_pools = dict()\n",
        "        pre_pools[\"layer_0\"] = x #original input image for the very last decoder stage\n",
        "\n",
        "        x = self.input_block(x)\n",
        "        pre_pools[\"layer_1\"] = x\n",
        "\n",
        "        for i, block in enumerate(self.down_blocks):\n",
        "            x = block(x)\n",
        "            if i < len(self.down_blocks) - 1: #ALL but the last one (layer4 output is going to bridge)\n",
        "                pre_pools[f\"layer_{i+2}\"] = x\n",
        "\n",
        "        return x, pre_pools\n",
        "# ----------------------------------------------------------------------------- #\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        # Decoder stages: in_channels_after_concat refers to channels after upsample + skip concat.\n",
        "        # Channels adapted for ResNet50 outputs.\n",
        "        # For 512x512 input, spatial resolutions are:\n",
        "        # Encoder: Input -> 128x128 (layer1, input_block) -> 64x64 (layer2) -> 32x32 (layer3) -> 16x16 (layer4/bridge)\n",
        "        # Decoder stages will upsample: 16x16 -> 32x32 -> 64x64 -> 128x128 -> 256x256 -> 512x512\n",
        "\n",
        "        #Up-block 1: Fuses deepest feature (from bridge/transformer) with ResNet.layer3 output\n",
        "        # up_x (from bridge) is 2048 channels, 16x16. Skip (pre_pools[\"layer_4\"]) is 1024 channels, 32x32.\n",
        "        # Concatenated: 2048 + 1024 = 3072 channels. Output: 1024 channels, 32x32 spatial.\n",
        "        self.up_block1 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=2048 + 1024, out_channels=1024,\n",
        "            up_conv_in_channels=2048, up_conv_out_channels=2048\n",
        "        )\n",
        "        # Up-block 2: Fuses output of up_block1 with ResNet.layer2 output\n",
        "        # up_x (from up_block1) is 1024 channels, 32x32. Skip (pre_pools[\"layer_3\"]) is 512 channels, 64x64.\n",
        "        # Concatenated: 1024 + 512 = 1536 channels. Output: 512 channels, 64x64 spatial.\n",
        "        self.up_block2 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=1024 + 512, out_channels=512,\n",
        "            up_conv_in_channels=1024, up_conv_out_channels=1024\n",
        "        )\n",
        "        # Up-block 3: Fuses output of up_block2 with ResNet.layer1 output\n",
        "        # up_x (from up_block2) is 512 channels, 64x64. Skip (pre_pools[\"layer_2\"]) is 256 channels, 128x128.\n",
        "        # Concatenated: 512 + 256 = 768 channels. Output: 256 channels, 128x128 spatial.\n",
        "        self.up_block3 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=512 + 256, out_channels=256,\n",
        "            up_conv_in_channels=512, up_conv_out_channels=512\n",
        "        )\n",
        "        # Up-block 4: Fuses output of up_block3 with Encoder's input_block output (after manual upsampling)\n",
        "        # up_x (from up_block3) is 256 channels, 128x128. Skip (pre_pools[\"layer_1\"]) is 64 channels, 128x128.\n",
        "        # After explicit upsampling of `pre_pools[\"layer_1\"]` to 256x256 in forward,\n",
        "        # Concatenated: 256 + 64 = 320 channels. Output: 128 channels, 256x256 spatial.\n",
        "        self.up_block4 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=256 + 64, out_channels=128,\n",
        "            up_conv_in_channels=256, up_conv_out_channels=256\n",
        "        )\n",
        "\n",
        "        # Final upsampling and convolution as per model summary\n",
        "        self.last_upsample = nn.Upsample(scale_factor=2.0, mode='bilinear', align_corners=False)\n",
        "        self.last_conv = ConvBlock(in_channels=128, out_channels=64, with_nonlinearity=True)\n",
        "\n",
        "        # Output layer to produce the binary tampering mask\n",
        "        self.out = nn.Conv2d(64, n_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, pre_pools):\n",
        "        # x is the deepest feature (from bridge/transformer path), 16x16 spatial\n",
        "        x = self.up_block1(x, pre_pools[\"layer_4\"]) # Output: 1024 channels, 32x32 spatial\n",
        "        x = self.up_block2(x, pre_pools[\"layer_3\"]) # Output: 512 channels, 64x64 spatial\n",
        "        x = self.up_block3(x, pre_pools[\"layer_2\"]) # Output: 256 channels, 128x128 spatial\n",
        "\n",
        "        #############CRITICAL FIX#####: Explicitly upsample pre_pools[\"layer_1\"] to 256x256 for up_block4\n",
        "        #this ensures spatial compatibility for concatenation with up_x (which becomes 256x256 after upsampling in UpBlock).\n",
        "        upsampled_skip_layer1 = F.interpolate(\n",
        "            pre_pools[\"layer_1\"],\n",
        "            size=(256, 256), #Target size for this skip connection\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "        x = self.up_block4(x, upsampled_skip_layer1) # \\Output: 128 channels, 256x256 spatial.\n",
        "\n",
        "        # Final upsampling and convolution to reach original input resolution (512x512)\n",
        "        x = self.last_upsample(x) # Upsamples 256x256 to 512x512\n",
        "        x = self.last_conv(x)      # Applies ConvBlock (128 channels -> 64 channels, 512x512)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------------------------------------------- #\n",
        "class FeatureSimilarityModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature Similarity Module (FSM) implementing block-wise Pearson correlation and percentile pooling.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(FeatureSimilarityModule, self).__init__()\n",
        "        self.K = 32 # Number of top similarity scores to select per block\n",
        "\n",
        "    def _pearson_correlation_coefficient(self, B_i, B_j):\n",
        "        \"\"\"\n",
        "        Calculates the Pearson correlation coefficient between two flattened feature blocks.\n",
        "        \"\"\"\n",
        "        B_i = B_i.float()\n",
        "        B_j = B_j.float()\n",
        "\n",
        "        mean_B_i = torch.mean(B_i)\n",
        "        mean_B_j = torch.mean(B_j)\n",
        "\n",
        "        std_B_i = torch.std(B_i, unbiased=False)\n",
        "        std_B_j = torch.std(B_j, unbiased=False)\n",
        "\n",
        "        if std_B_i == 0 or std_B_j == 0:\n",
        "            return torch.tensor(0.0, device=B_i.device)\n",
        "\n",
        "        normalized_B_i = (B_i - mean_B_i) / std_B_i\n",
        "        normalized_B_j = (B_j - mean_B_j) / std_B_j\n",
        "\n",
        "        correlation = torch.dot(normalized_B_i, normalized_B_j) / B_i.numel()\n",
        "        return correlation\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # Expected input `feature_map` is the \"image-like feature F\" from transformer output,\n",
        "        # anticipated to be `(batch_size, channels=512, H=256, W=256)`.\n",
        "        batch_size, channels, H, W = feature_map.shape\n",
        "\n",
        "        block_size_spatial = 16 #Original Paper specifies 16x16 non-overlapping blocks\n",
        "        num_blocks_h = H // block_size_spatial\n",
        "        num_blocks_w = W // block_size_spatial\n",
        "        num_blocks = num_blocks_h * num_blocks_w #Results in 256 total blocks for 256x256 input\n",
        "\n",
        "        #Self-Correlation Calculation Block: For feature map into blocks\n",
        "        blocks_unfolded = feature_map.unfold(2, block_size_spatial, block_size_spatial).unfold(3, block_size_spatial, block_size_spatial)\n",
        "\n",
        "        #Flatten each block into a 1D feature vector for similarity computation\n",
        "        blocks_flat = blocks_unfolded.permute(0, 2, 3, 1, 4, 5).contiguous().view(\n",
        "            batch_size, num_blocks, -1\n",
        "        )\n",
        "\n",
        "        #Compute pairwise Pearson correlation coefficients, forming a similarity matrix\n",
        "        similarity_matrix = torch.zeros(batch_size, num_blocks, num_blocks, device=feature_map.device)\n",
        "        for b in range(batch_size):\n",
        "            for i in range(num_blocks):\n",
        "                for j in range(num_blocks):\n",
        "                    similarity_matrix[b, i, j] = self._pearson_correlation_coefficient(blocks_flat[b, i], blocks_flat[b, j])\n",
        "\n",
        "        #Percentile Pooling Block: Select top K similarities\n",
        "        percentile_scores = torch.zeros(batch_size, num_blocks, self.K, device=feature_map.device)\n",
        "        for b in range(batch_size):\n",
        "            for i in range(num_blocks):\n",
        "                sorted_scores = torch.sort(similarity_matrix[b, i], descending=True).values\n",
        "\n",
        "                if self.K <= sorted_scores.shape[0]:\n",
        "                    percentile_scores[b, i] = sorted_scores[:self.K]\n",
        "                else:\n",
        "                    percentile_scores[b, i, :sorted_scores.shape[0]] = sorted_scores\n",
        "\n",
        "        return percentile_scores ####Output shape: (batch_size, 256, self.K)\n",
        "\n",
        "#Adaptive Transformer Components\n",
        "# Custom Adaptive Multi-Head Self-Attention for the transformer's core innovation as given in the paper\n",
        "class AdaptiveMultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Adaptive Multi-Head Self-Attention (AdaptiveMSA) module.\n",
        "    Implements the Dual-Path Adaptive Attention Mechanism (DPAAM) as described in Algorithm 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads # d_k = d_v = d_model / h\n",
        "\n",
        "        if self.head_dim * num_heads != self.embed_dim:\n",
        "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
        "\n",
        "        #Linear transformations for Query, Key, Value\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        #Learnable parameters for DPAAM: S1, S2, b1, b2, epsilon, delta\n",
        "        self.S1 = nn.Parameter(torch.ones(embed_dim)) #Initialized to 1 for identity-like start\n",
        "        self.S2 = nn.Parameter(torch.ones(embed_dim)) #Applied element-wise across the embedding dimension\n",
        "\n",
        "        self.b1 = nn.Parameter(torch.zeros(embed_dim)) #Learnable bias vectors\n",
        "        self.b2 = nn.Parameter(torch.zeros(embed_dim))\n",
        "\n",
        "        self.epsilon = nn.Parameter(torch.tensor(0.5)) #Weighting factors (initially balanced)\n",
        "        self.delta = nn.Parameter(torch.tensor(0.5))\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        batch_size, seq_len, embed_dim = query.size()\n",
        "\n",
        "        #Project Query, Key, Value\n",
        "        Q = self.q_proj(query)\n",
        "        K = self.k_proj(key)\n",
        "        V = self.v_proj(value)\n",
        "\n",
        "        #Reshape for multi-head attention: (B, SeqLen, EmbedDim) -> (B, NumHeads, SeqLen, HeadDim)\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        #Compute Attention Weights (QK^T / sqrt(d_k))\n",
        "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        #Apply Softmax to get probabilities\n",
        "        attention_probs = F.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        #Calculate Head_i' (standard attention output before adaptivity)\n",
        "        Head_prime = torch.matmul(attention_probs, V)\n",
        "\n",
        "        #Reshape Head_prime back to (batch_size, seq_len, embed_dim) for element-wise DPAAM ops\n",
        "        Head_prime_combined = Head_prime.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        #Apply DPAAM (Dual-Path Adaptive Attention Mechanism)\n",
        "        M1_Head = self.S1 * Head_prime_combined + self.b1 # Element-wise scale and bias\n",
        "        M2_Head = self.S2 * Head_prime_combined + self.b2 # Element-wise scale and bias\n",
        "\n",
        "        #Apply sigmoid to epsilon and delta to constrain them to (0,1) as per paper\n",
        "        epsilon_val = torch.sigmoid(self.epsilon)\n",
        "        delta_val = torch.sigmoid(self.delta)\n",
        "\n",
        "        #Combine the mapped heads with epsilon and delta weights\n",
        "        adaptive_heads_combined = epsilon_val * M1_Head + delta_val * M2_Head\n",
        "\n",
        "        #Final output projection (W_0)\n",
        "        output = self.out_proj(adaptive_heads_combined)\n",
        "        return output\n",
        "\n",
        "#AdaptiveTransformerLayer (UPDATED to use custom AdaptiveMSA)\n",
        "class AdaptiveTransformerLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of the Adaptive Transformer Encoder, incorporating AdaptiveMSA and MLP.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super().__init__()\n",
        "        #Using the custom AdaptiveMultiHeadSelfAttention module\n",
        "        self.adaptive_mhsa = AdaptiveMultiHeadSelfAttention(embed_dim=dim, num_heads=num_heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim) #layer Normalization\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.mlp = nn.Sequential( #MLP (Feedforward Network)\n",
        "            nn.Linear(dim, dim * 4), #Common expansion factor of 4\n",
        "            nn.GELU(), #GELU activation is common in modern transformers\n",
        "            nn.Linear(dim * 4, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x is expected as token sequence: (batch_size, sequence_length, embedding_dimension)\n",
        "        attn_output = self.adaptive_mhsa(x, x, x) #Custom MHSA call\n",
        "        x = self.norm1(x + attn_output) #Add residual connection and apply LayerNorm\n",
        "\n",
        "        x_mlp = self.mlp(x)\n",
        "        x = self.norm2(x + x_mlp) #Add residual connection and apply LayerNorm\n",
        "        return x\n",
        "\n",
        "#AdaptiveTransformerEncoder (Uses AdaptiveTransformerLayer)\n",
        "class AdaptiveTransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Adaptive Transformer Encoder, stacking multiple AdaptiveTransformerLayers.\n",
        "    Handles Tokenization and Positional Embedding for CNN features.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.dim = dim #Transformer's embedding dimension\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([AdaptiveTransformerLayer(dim, num_heads) for _ in range(num_layers)])\n",
        "\n",
        "        #Tokenization: Projects CNN encoder's C4 output channels (2048) to transformer's `dim`\n",
        "        self.proj_to_dim = nn.Conv2d(2048, dim, kernel_size=1)\n",
        "\n",
        "        #Learnable Positional Embedding\n",
        "        #Assuming fixed 16x16 spatial size for tokens (256 tokens) from a 512x512 input.\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 16 * 16, dim))\n",
        "\n",
        "    def forward(self, feature_map_c4):\n",
        "        # `feature_map_c4` is the deepest feature from the CNN Encoder (Bridge input):\n",
        "        #Expected shape: (batch_size, 2048, H_spatial, W_spatial) (e.g., 16x16 for 512x512 input)\n",
        "\n",
        "        x = self.proj_to_dim(feature_map_c4) # Output: (B, dim, H_spatial, W_spatial)\n",
        "        x = x.flatten(2).permute(0, 2, 1) # (B, H_spatial * W_spatial, dim) -> e.g., (B, 256, dim)\n",
        "\n",
        "        if x.shape[1] != self.pos_embedding.shape[1]:\n",
        "            raise ValueError(f\"Positional embedding sequence length mismatch. Expected {self.pos_embedding.shape[1]}, got {x.shape[1]}\")\n",
        "\n",
        "        x = x + self.pos_embedding #Add learnable positional encoding to tokens\n",
        "\n",
        "        #Pass through the stack of transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x #Output tokens: (batch_size, sequence_length, embedding_dimension)\n",
        "\n",
        "\n",
        "#Main TransCMFDBaseline Model\n",
        "\n",
        "class TransCMFDBaseline(nn.Module):\n",
        "    \"\"\"\n",
        "    The complete TransCMFD model architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.bridge = Bridge(2048, 2048)\n",
        "\n",
        "        #Adaptive Transformer Encoder setup\n",
        "        transformer_dim = 512\n",
        "        transformer_heads = 8\n",
        "        transformer_layers = 2 #Reduced to 2 layers as per plan for time constraint\n",
        "\n",
        "        self.adaptive_transformer_encoder = AdaptiveTransformerEncoder(\n",
        "            dim=transformer_dim,\n",
        "            num_heads=transformer_heads,\n",
        "            num_layers=transformer_layers\n",
        "        )\n",
        "\n",
        "        #Transforms Transformer Output (tokens) to FSM Input (spatial feature map F)\n",
        "        #Transformer output: (B, seq_len=256, dim=512)\n",
        "        #FSM expects input `F` as: (B, channels=512, H=256, W=256)\n",
        "\n",
        "        self.transformer_output_to_fsm_input = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.fsm = FeatureSimilarityModule() # Feature Similarity Module\n",
        "\n",
        "        #Transforms FSM output back to a spatial feature map for fusion with decoder path\n",
        "        #FSM output: (B, num_blocks=256, K=32)\n",
        "        #This layer takes `(B, K, 16, 16)` (after internal reshape) and projects channels `K` to `2048`\n",
        "        #to match `x_bridge` for fusion.\n",
        "        self.fsm_output_fusion_transform = nn.Conv2d(self.fsm.K, 2048, kernel_size=1)\n",
        "\n",
        "        self.decoder = Decoder(n_classes=n_classes) # CNN Decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1.CNN Encoder: Extracts local features and provides skip connections\n",
        "        encoder_output_c4, pre_pools = self.encoder(x)\n",
        "\n",
        "        # 2.Bridge: Connects encoder's deepest feature to transformer/decoder path\n",
        "        x_bridge = self.bridge(encoder_output_c4) # (B, 2048, 16, 16) for 512x512 input\n",
        "\n",
        "        # 3.Adaptive Transformer Encoder: Learns global representations from tokens\n",
        "        transformer_output_tokens = self.adaptive_transformer_encoder(encoder_output_c4) # Output: (B, 256, dim=512)\n",
        "\n",
        "        # 4.Transform Transformer Output to FSM Input (\"image-like feature F\")\n",
        "        # Reshape transformer tokens `(B, 256, 512)` into a spatial feature map `(B, 512, 16, 16)`\n",
        "        fsm_input_spatial_reshaped = transformer_output_tokens.permute(0, 2, 1).contiguous().view(\n",
        "            transformer_output_tokens.size(0),\n",
        "            self.adaptive_transformer_encoder.dim, # Channels (512)\n",
        "            16, 16 # Spatial dimensions, derived from sequence length (256 = 16*16)\n",
        "        )\n",
        "        #Upsample this `(B, 512, 16, 16)` to `(B, 512, 256, 256)` as FSM expects\n",
        "        fsm_input = self.transformer_output_to_fsm_input(fsm_input_spatial_reshaped)\n",
        "\n",
        "        # 5.Feature Similarity Module (FSM): Identifies similar regions\n",
        "        fsm_output_raw = self.fsm(fsm_input) # Output: (B, 256, K=32)\n",
        "\n",
        "        # 6.Fuse FSM output with decoder path\n",
        "        # Reshape FSM output `(B, 256, K=32)` into `(B, K, 16, 16)` for spatial compatibility\n",
        "        fsm_output_spatial_for_fusion = fsm_output_raw.permute(0, 2, 1).contiguous().view(\n",
        "            fsm_output_raw.size(0), self.fsm.K, 16, 16\n",
        "        )\n",
        "        #Project `K` channels to `2048` channels to match `x_bridge` for element-wise addition (fusion)\n",
        "        fsm_output_fused = self.fsm_output_fusion_transform(fsm_output_spatial_for_fusion)\n",
        "\n",
        "        #Fusion: Add FSM contribution to the bridge output (main decoder input)\n",
        "        fused_decoder_input = x_bridge + fsm_output_fused\n",
        "\n",
        "        #7.CNN Decoder: Reconstructs the mask from fused features\n",
        "        output = self.decoder(fused_decoder_input, pre_pools)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "#Dummy Data Testing\n",
        "#This block will test the model's forward pass and print its structure.\n",
        "if __name__ == '__main__':\n",
        "    print(\"Initializing TransCMFDBaseline model...\")\n",
        "    model = TransCMFDBaseline().cuda() # Instantiate model and move to GPU\n",
        "    print(\"TransCMFDBaseline model successfully loaded on cuda.\")\n",
        "\n",
        "    #Print the model summary to verify custom modules that I have used in this architecture\n",
        "    print(\"\\n--- Model Architecture Summary ---\")\n",
        "    print(model)\n",
        "    print(\"----------------------------------\\n\")\n",
        "\n",
        "    dummy_input = torch.rand((2, 3, 512, 512)).cuda() #Dummy input: Batch size 2, 3 channels (RGB), 512x512 resolution of image\n",
        "    print(f\"Dummy input shape: {dummy_input.shape}\")\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient computation for faster dummy pass (just for dummy input usage)\n",
        "        output = model(dummy_input)\n",
        "\n",
        "    print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHc_QIn9e6IS",
        "outputId": "100b2906-0583-4206-cfa6-94ad89759439"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing TransCMFDBaseline model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 226MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransCMFDBaseline model successfully loaded on cuda.\n",
            "\n",
            "--- Model Architecture Summary ---\n",
            "TransCMFDBaseline(\n",
            "  (encoder): Encoder(\n",
            "    (input_block): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (down_blocks): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (bridge): Bridge(\n",
            "    (bridge): Sequential(\n",
            "      (0): ConvBlock(\n",
            "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): ConvBlock(\n",
            "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (adaptive_transformer_encoder): AdaptiveTransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x AdaptiveTransformerLayer(\n",
            "        (adaptive_mhsa): AdaptiveMultiHeadSelfAttention(\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (proj_to_dim): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (transformer_output_to_fsm_input): Upsample(size=(256, 256), mode='bilinear')\n",
            "  (fsm): FeatureSimilarityModule()\n",
            "  (fsm_output_fusion_transform): Conv2d(32, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (decoder): Decoder(\n",
            "    (up_block1): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(3072, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (up_block2): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (up_block3): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (up_block4): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (last_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (last_conv): ConvBlock(\n",
            "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (gn): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (out): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n",
            "----------------------------------\n",
            "\n",
            "Dummy input shape: torch.Size([2, 3, 512, 512])\n",
            "Output shape: torch.Size([2, 2, 512, 512])\n"
          ]
        }
      ]
    }
  ]
}
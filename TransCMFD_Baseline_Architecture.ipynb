{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMiK1kwS+DZ4defCWnG9trw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5055464f662c4527a9d94293c51f4163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e609002a4e274ea5a7e82b5ce6e56041",
              "IPY_MODEL_ef3198b99c194ae48c2f41e8f0812bd5",
              "IPY_MODEL_6a29e2017fb74f9eb7504f35480e2994"
            ],
            "layout": "IPY_MODEL_a0f1709b88e84eb3aefda4daa986bb04"
          }
        },
        "e609002a4e274ea5a7e82b5ce6e56041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1193b765558d4488aca27255ca4c4a7b",
            "placeholder": "​",
            "style": "IPY_MODEL_d91b291aa71949028e4f6cc1a569bccc",
            "value": "Training Epoch 1:   0%"
          }
        },
        "ef3198b99c194ae48c2f41e8f0812bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_312a28d53df74d5089b5f1fdab09d69e",
            "max": 1603,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bd13eb4afdc4d5eba08e17184efd155",
            "value": 0
          }
        },
        "6a29e2017fb74f9eb7504f35480e2994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da34faa103124a01bf667fcae0352d36",
            "placeholder": "​",
            "style": "IPY_MODEL_689bc355db89414ebbdd76c959a43e43",
            "value": " 0/1603 [00:00&lt;?, ?it/s]"
          }
        },
        "a0f1709b88e84eb3aefda4daa986bb04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1193b765558d4488aca27255ca4c4a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d91b291aa71949028e4f6cc1a569bccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "312a28d53df74d5089b5f1fdab09d69e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bd13eb4afdc4d5eba08e17184efd155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da34faa103124a01bf667fcae0352d36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "689bc355db89414ebbdd76c959a43e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitajain523/TransCMFD_Copy_Move_Forgery_Detection/blob/main/TransCMFD_Baseline_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n",
        "#mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "m7lc2v1yW1OG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3412b8-c42c-4877-99e1-41bd30452e7f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA device name: NVIDIA A100-SXM4-40GB\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#  Helper Modules\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
        "        self.gn = nn.GroupNorm(32, out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.with_nonlinearity = with_nonlinearity\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.gn(x)\n",
        "        if self.with_nonlinearity:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "class Bridge(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.bridge = nn.Sequential(\n",
        "            ConvBlock(in_channels, out_channels),\n",
        "            ConvBlock(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bridge(x)\n",
        "\n",
        "# ----------------------------------------------------------------------------- #\n",
        "#upblock\n",
        "class UpBlockForUNetWithResNet50(nn.Module):\n",
        "    #Consists of Upsample ->(Concatenation with skip connection)->ConvBlock.\n",
        "    def __init__(self, in_channels_after_concat, out_channels, up_conv_in_channels, up_conv_out_channels,\n",
        "                 upsampling_method=\"bilinear\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsampling_method = upsampling_method\n",
        "        if upsampling_method == \"conv_transpose\":\n",
        "            self.upsample_layer = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
        "        elif upsampling_method == \"bilinear\":\n",
        "            self.upsample_layer = nn.Upsample(scale_factor=2.0, mode='bilinear', align_corners=False)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported upsampling_method\")\n",
        "\n",
        "        self.conv_block = ConvBlock(in_channels_after_concat, out_channels)\n",
        "\n",
        "    def forward(self, up_x, down_x):\n",
        "        up_x = self.upsample_layer(up_x)\n",
        "\n",
        "        if up_x.shape[2] != down_x.shape[2] or up_x.shape[3] != down_x.shape[3]:\n",
        "            up_x = F.interpolate(up_x, size=(down_x.shape[2], down_x.shape[3]), mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = torch.cat([up_x, down_x], 1)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------- #\n",
        "\n",
        "#CNN Encoder\n",
        "class Encoder(nn.Module):\n",
        "    #CNN Encoder based on a pre-trained ResNet-50, using the channels for resnet 50 itself.\n",
        "    DEPTH = 6\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        resnet = torchvision.models.resnet.resnet50(pretrained=True)##using pretrained weights\n",
        "\n",
        "        self.input_block = nn.Sequential(*list(resnet.children()))[:4]\n",
        "\n",
        "        down_blocks = []\n",
        "        for bottleneck_stage in list(resnet.children())[4:]:\n",
        "            if isinstance(bottleneck_stage, nn.Sequential):\n",
        "                down_blocks.append(bottleneck_stage)\n",
        "        self.down_blocks = nn.ModuleList(down_blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre_pools = dict()\n",
        "        pre_pools[\"layer_0\"] = x #original input image for the very last decoder stage\n",
        "\n",
        "        x = self.input_block(x)\n",
        "        pre_pools[\"layer_1\"] = x\n",
        "\n",
        "        for i, block in enumerate(self.down_blocks):\n",
        "            x = block(x)\n",
        "            if i < len(self.down_blocks) - 1: #ALL but the last one (layer4 output is going to bridge)\n",
        "                pre_pools[f\"layer_{i+2}\"] = x\n",
        "\n",
        "        return x, pre_pools\n",
        "# ----------------------------------------------------------------------------- #\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        # Decoder stages: in_channels_after_concat refers to channels after upsample + skip concat.\n",
        "        # Channels adapted for ResNet50 outputs.\n",
        "        # For 512x512 input, spatial resolutions are:\n",
        "        # Encoder: Input -> 128x128 (layer1, input_block) -> 64x64 (layer2) -> 32x32 (layer3) -> 16x16 (layer4/bridge)\n",
        "        # Decoder stages will upsample: 16x16 -> 32x32 -> 64x64 -> 128x128 -> 256x256 -> 512x512\n",
        "\n",
        "        #Up-block 1: Fuses deepest feature (from bridge/transformer) with ResNet.layer3 output\n",
        "        # up_x (from bridge) is 2048 channels, 16x16. Skip (pre_pools[\"layer_4\"]) is 1024 channels, 32x32.\n",
        "        # Concatenated: 2048 + 1024 = 3072 channels. Output: 1024 channels, 32x32 spatial.\n",
        "        self.up_block1 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=2048 + 1024, out_channels=1024,\n",
        "            up_conv_in_channels=2048, up_conv_out_channels=2048\n",
        "        )\n",
        "        # Up-block 2: Fuses output of up_block1 with ResNet.layer2 output\n",
        "        # up_x (from up_block1) is 1024 channels, 32x32. Skip (pre_pools[\"layer_3\"]) is 512 channels, 64x64.\n",
        "        # Concatenated: 1024 + 512 = 1536 channels. Output: 512 channels, 64x64 spatial.\n",
        "        self.up_block2 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=1024 + 512, out_channels=512,\n",
        "            up_conv_in_channels=1024, up_conv_out_channels=1024\n",
        "        )\n",
        "        # Up-block 3: Fuses output of up_block2 with ResNet.layer1 output\n",
        "        # up_x (from up_block2) is 512 channels, 64x64. Skip (pre_pools[\"layer_2\"]) is 256 channels, 128x128.\n",
        "        # Concatenated: 512 + 256 = 768 channels. Output: 256 channels, 128x128 spatial.\n",
        "        self.up_block3 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=512 + 256, out_channels=256,\n",
        "            up_conv_in_channels=512, up_conv_out_channels=512\n",
        "        )\n",
        "        # Up-block 4: Fuses output of up_block3 with Encoder's input_block output (after manual upsampling)\n",
        "        # up_x (from up_block3) is 256 channels, 128x128. Skip (pre_pools[\"layer_1\"]) is 64 channels, 128x128.\n",
        "        # After explicit upsampling of `pre_pools[\"layer_1\"]` to 256x256 in forward,\n",
        "        # Concatenated: 256 + 64 = 320 channels. Output: 128 channels, 256x256 spatial.\n",
        "        self.up_block4 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=256 + 64, out_channels=128,\n",
        "            up_conv_in_channels=256, up_conv_out_channels=256\n",
        "        )\n",
        "\n",
        "        # Final upsampling and convolution as per model summary\n",
        "        self.last_upsample = nn.Upsample(scale_factor=2.0, mode='bilinear', align_corners=False)\n",
        "        self.last_conv = ConvBlock(in_channels=128, out_channels=64, with_nonlinearity=True)\n",
        "\n",
        "        # Output layer to produce the binary tampering mask\n",
        "        self.out = nn.Conv2d(64, n_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, pre_pools):\n",
        "        # x is the deepest feature (from bridge/transformer path), 16x16 spatial\n",
        "        x = self.up_block1(x, pre_pools[\"layer_4\"]) # Output: 1024 channels, 32x32 spatial\n",
        "        x = self.up_block2(x, pre_pools[\"layer_3\"]) # Output: 512 channels, 64x64 spatial\n",
        "        x = self.up_block3(x, pre_pools[\"layer_2\"]) # Output: 256 channels, 128x128 spatial\n",
        "\n",
        "        #############CRITICAL FIX#####: Explicitly upsample pre_pools[\"layer_1\"] to 256x256 for up_block4\n",
        "        #this ensures spatial compatibility for concatenation with up_x (which becomes 256x256 after upsampling in UpBlock).\n",
        "        upsampled_skip_layer1 = F.interpolate(\n",
        "            pre_pools[\"layer_1\"],\n",
        "            size=(256, 256), #Target size for this skip connection\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "        x = self.up_block4(x, upsampled_skip_layer1) # \\Output: 128 channels, 256x256 spatial.\n",
        "\n",
        "        # Final upsampling and convolution to reach original input resolution (512x512)\n",
        "        x = self.last_upsample(x) # Upsamples 256x256 to 512x512\n",
        "        x = self.last_conv(x)      # Applies ConvBlock (128 channels -> 64 channels, 512x512)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------------------------------------------- #\n",
        "class FeatureSimilarityModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature Similarity Module (FSM) implementing block-wise Pearson correlation and percentile pooling.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(FeatureSimilarityModule, self).__init__()\n",
        "        self.K = 32 # Number of top similarity scores to select per block\n",
        "\n",
        "    def _pearson_correlation_coefficient(self, B_i, B_j):\n",
        "        \"\"\"\n",
        "        Calculates the Pearson correlation coefficient between two flattened feature blocks.\n",
        "        \"\"\"\n",
        "        B_i = B_i.float()\n",
        "        B_j = B_j.float()\n",
        "\n",
        "        mean_B_i = torch.mean(B_i)\n",
        "        mean_B_j = torch.mean(B_j)\n",
        "\n",
        "        std_B_i = torch.std(B_i, unbiased=False)\n",
        "        std_B_j = torch.std(B_j, unbiased=False)\n",
        "\n",
        "        if std_B_i == 0 or std_B_j == 0:\n",
        "            return torch.tensor(0.0, device=B_i.device)\n",
        "\n",
        "        normalized_B_i = (B_i - mean_B_i) / std_B_i\n",
        "        normalized_B_j = (B_j - mean_B_j) / std_B_j\n",
        "\n",
        "        correlation = torch.dot(normalized_B_i, normalized_B_j) / B_i.numel()\n",
        "        return correlation\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # Expected input `feature_map` is the \"image-like feature F\" from transformer output,\n",
        "        # anticipated to be `(batch_size, channels=512, H=256, W=256)`.\n",
        "        batch_size, channels, H, W = feature_map.shape\n",
        "\n",
        "        block_size_spatial = 16 #Original Paper specifies 16x16 non-overlapping blocks\n",
        "        num_blocks_h = H // block_size_spatial\n",
        "        num_blocks_w = W // block_size_spatial\n",
        "        num_blocks = num_blocks_h * num_blocks_w #Results in 256 total blocks for 256x256 input\n",
        "\n",
        "        #Self-Correlation Calculation Block: For feature map into blocks\n",
        "        blocks_unfolded = feature_map.unfold(2, block_size_spatial, block_size_spatial).unfold(3, block_size_spatial, block_size_spatial)\n",
        "\n",
        "        #Flatten each block into a 1D feature vector for similarity computation\n",
        "        blocks_flat = blocks_unfolded.permute(0, 2, 3, 1, 4, 5).contiguous().view(\n",
        "            batch_size, num_blocks, -1\n",
        "        )\n",
        "\n",
        "        #Compute pairwise Pearson correlation coefficients, forming a similarity matrix\n",
        "        similarity_matrix = torch.zeros(batch_size, num_blocks, num_blocks, device=feature_map.device)\n",
        "        for b in range(batch_size):\n",
        "            for i in range(num_blocks):\n",
        "                for j in range(num_blocks):\n",
        "                    similarity_matrix[b, i, j] = self._pearson_correlation_coefficient(blocks_flat[b, i], blocks_flat[b, j])\n",
        "\n",
        "        #Percentile Pooling Block: Select top K similarities\n",
        "        percentile_scores = torch.zeros(batch_size, num_blocks, self.K, device=feature_map.device)\n",
        "        for b in range(batch_size):\n",
        "            for i in range(num_blocks):\n",
        "                sorted_scores = torch.sort(similarity_matrix[b, i], descending=True).values\n",
        "\n",
        "                if self.K <= sorted_scores.shape[0]:\n",
        "                    percentile_scores[b, i] = sorted_scores[:self.K]\n",
        "                else:\n",
        "                    percentile_scores[b, i, :sorted_scores.shape[0]] = sorted_scores\n",
        "\n",
        "        return percentile_scores ####Output shape: (batch_size, 256, self.K)\n",
        "\n",
        "#Adaptive Transformer Components\n",
        "# Custom Adaptive Multi-Head Self-Attention for the transformer's core innovation as given in the paper\n",
        "class AdaptiveMultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Adaptive Multi-Head Self-Attention (AdaptiveMSA) module.\n",
        "    Implements the Dual-Path Adaptive Attention Mechanism (DPAAM) as described in Algorithm 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads # d_k = d_v = d_model / h\n",
        "\n",
        "        if self.head_dim * num_heads != self.embed_dim:\n",
        "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
        "\n",
        "        #Linear transformations for Query, Key, Value\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        #Learnable parameters for DPAAM: S1, S2, b1, b2, epsilon, delta\n",
        "        self.S1 = nn.Parameter(torch.ones(embed_dim)) #Initialized to 1 for identity-like start\n",
        "        self.S2 = nn.Parameter(torch.ones(embed_dim)) #Applied element-wise across the embedding dimension\n",
        "\n",
        "        self.b1 = nn.Parameter(torch.zeros(embed_dim)) #Learnable bias vectors\n",
        "        self.b2 = nn.Parameter(torch.zeros(embed_dim))\n",
        "\n",
        "        self.epsilon = nn.Parameter(torch.tensor(0.5)) #Weighting factors (initially balanced)\n",
        "        self.delta = nn.Parameter(torch.tensor(0.5))\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        batch_size, seq_len, embed_dim = query.size()\n",
        "\n",
        "        #Project Query, Key, Value\n",
        "        Q = self.q_proj(query)\n",
        "        K = self.k_proj(key)\n",
        "        V = self.v_proj(value)\n",
        "\n",
        "        #Reshape for multi-head attention: (B, SeqLen, EmbedDim) -> (B, NumHeads, SeqLen, HeadDim)\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        #Compute Attention Weights (QK^T / sqrt(d_k))\n",
        "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        #Apply Softmax to get probabilities\n",
        "        attention_probs = F.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        #Calculate Head_i' (standard attention output before adaptivity)\n",
        "        Head_prime = torch.matmul(attention_probs, V)\n",
        "\n",
        "        #Reshape Head_prime back to (batch_size, seq_len, embed_dim) for element-wise DPAAM ops\n",
        "        Head_prime_combined = Head_prime.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        #Apply DPAAM (Dual-Path Adaptive Attention Mechanism)\n",
        "        M1_Head = self.S1 * Head_prime_combined + self.b1 # Element-wise scale and bias\n",
        "        M2_Head = self.S2 * Head_prime_combined + self.b2 # Element-wise scale and bias\n",
        "\n",
        "        #Apply sigmoid to epsilon and delta to constrain them to (0,1) as per paper\n",
        "        epsilon_val = torch.sigmoid(self.epsilon)\n",
        "        delta_val = torch.sigmoid(self.delta)\n",
        "\n",
        "        #Combine the mapped heads with epsilon and delta weights\n",
        "        adaptive_heads_combined = epsilon_val * M1_Head + delta_val * M2_Head\n",
        "\n",
        "        #Final output projection (W_0)\n",
        "        output = self.out_proj(adaptive_heads_combined)\n",
        "        return output\n",
        "\n",
        "#AdaptiveTransformerLayer (UPDATED to use custom AdaptiveMSA)\n",
        "class AdaptiveTransformerLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of the Adaptive Transformer Encoder, incorporating AdaptiveMSA and MLP.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super().__init__()\n",
        "        #Using the custom AdaptiveMultiHeadSelfAttention module\n",
        "        self.adaptive_mhsa = AdaptiveMultiHeadSelfAttention(embed_dim=dim, num_heads=num_heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim) #layer Normalization\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.mlp = nn.Sequential( #MLP (Feedforward Network)\n",
        "            nn.Linear(dim, dim * 4), #Common expansion factor of 4\n",
        "            nn.GELU(), #GELU activation is common in modern transformers\n",
        "            nn.Linear(dim * 4, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x is expected as token sequence: (batch_size, sequence_length, embedding_dimension)\n",
        "        attn_output = self.adaptive_mhsa(x, x, x) #Custom MHSA call\n",
        "        x = self.norm1(x + attn_output) #Add residual connection and apply LayerNorm\n",
        "\n",
        "        x_mlp = self.mlp(x)\n",
        "        x = self.norm2(x + x_mlp) #Add residual connection and apply LayerNorm\n",
        "        return x\n",
        "\n",
        "#AdaptiveTransformerEncoder (Uses AdaptiveTransformerLayer)\n",
        "class AdaptiveTransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Adaptive Transformer Encoder, stacking multiple AdaptiveTransformerLayers.\n",
        "    Handles Tokenization and Positional Embedding for CNN features.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.dim = dim #Transformer's embedding dimension\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([AdaptiveTransformerLayer(dim, num_heads) for _ in range(num_layers)])\n",
        "\n",
        "        #Tokenization: Projects CNN encoder's C4 output channels (2048) to transformer's `dim`\n",
        "        self.proj_to_dim = nn.Conv2d(2048, dim, kernel_size=1)\n",
        "\n",
        "        #Learnable Positional Embedding\n",
        "        #Assuming fixed 16x16 spatial size for tokens (256 tokens) from a 512x512 input.\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 16 * 16, dim))\n",
        "\n",
        "    def forward(self, feature_map_c4):\n",
        "        # `feature_map_c4` is the deepest feature from the CNN Encoder (Bridge input):\n",
        "        #Expected shape: (batch_size, 2048, H_spatial, W_spatial) (e.g., 16x16 for 512x512 input)\n",
        "\n",
        "        x = self.proj_to_dim(feature_map_c4) # Output: (B, dim, H_spatial, W_spatial)\n",
        "        x = x.flatten(2).permute(0, 2, 1) # (B, H_spatial * W_spatial, dim) -> e.g., (B, 256, dim)\n",
        "\n",
        "        if x.shape[1] != self.pos_embedding.shape[1]:\n",
        "            raise ValueError(f\"Positional embedding sequence length mismatch. Expected {self.pos_embedding.shape[1]}, got {x.shape[1]}\")\n",
        "\n",
        "        x = x + self.pos_embedding #Add learnable positional encoding to tokens\n",
        "\n",
        "        #Pass through the stack of transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x #Output tokens: (batch_size, sequence_length, embedding_dimension)\n",
        "\n",
        "\n",
        "#Main TransCMFDBaseline Model\n",
        "\n",
        "class TransCMFDBaseline(nn.Module):\n",
        "    \"\"\"\n",
        "    The complete TransCMFD model architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.bridge = Bridge(2048, 2048)\n",
        "\n",
        "        #Adaptive Transformer Encoder setup\n",
        "        transformer_dim = 512\n",
        "        transformer_heads = 8\n",
        "        transformer_layers = 2 #Reduced to 2 layers as per plan for time constraint\n",
        "\n",
        "        self.adaptive_transformer_encoder = AdaptiveTransformerEncoder(\n",
        "            dim=transformer_dim,\n",
        "            num_heads=transformer_heads,\n",
        "            num_layers=transformer_layers\n",
        "        )\n",
        "\n",
        "        #Transforms Transformer Output (tokens) to FSM Input (spatial feature map F)\n",
        "        #Transformer output: (B, seq_len=256, dim=512)\n",
        "        #FSM expects input `F` as: (B, channels=512, H=256, W=256)\n",
        "\n",
        "        self.transformer_output_to_fsm_input = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.fsm = FeatureSimilarityModule() # Feature Similarity Module\n",
        "\n",
        "        #Transforms FSM output back to a spatial feature map for fusion with decoder path\n",
        "        #FSM output: (B, num_blocks=256, K=32)\n",
        "        #This layer takes `(B, K, 16, 16)` (after internal reshape) and projects channels `K` to `2048`\n",
        "        #to match `x_bridge` for fusion.\n",
        "        self.fsm_output_fusion_transform = nn.Conv2d(self.fsm.K, 2048, kernel_size=1)\n",
        "\n",
        "        self.decoder = Decoder(n_classes=n_classes) # CNN Decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1.CNN Encoder: Extracts local features and provides skip connections\n",
        "        encoder_output_c4, pre_pools = self.encoder(x)\n",
        "\n",
        "        # 2.Bridge: Connects encoder's deepest feature to transformer/decoder path\n",
        "        x_bridge = self.bridge(encoder_output_c4) # (B, 2048, 16, 16) for 512x512 input\n",
        "\n",
        "        # 3.Adaptive Transformer Encoder: Learns global representations from tokens\n",
        "        transformer_output_tokens = self.adaptive_transformer_encoder(encoder_output_c4) # Output: (B, 256, dim=512)\n",
        "\n",
        "        # 4.Transform Transformer Output to FSM Input (\"image-like feature F\")\n",
        "        # Reshape transformer tokens `(B, 256, 512)` into a spatial feature map `(B, 512, 16, 16)`\n",
        "        fsm_input_spatial_reshaped = transformer_output_tokens.permute(0, 2, 1).contiguous().view(\n",
        "            transformer_output_tokens.size(0),\n",
        "            self.adaptive_transformer_encoder.dim, # Channels (512)\n",
        "            16, 16 # Spatial dimensions, derived from sequence length (256 = 16*16)\n",
        "        )\n",
        "        #Upsample this `(B, 512, 16, 16)` to `(B, 512, 256, 256)` as FSM expects\n",
        "        fsm_input = self.transformer_output_to_fsm_input(fsm_input_spatial_reshaped)\n",
        "\n",
        "        # 5.Feature Similarity Module (FSM): Identifies similar regions\n",
        "        fsm_output_raw = self.fsm(fsm_input) # Output: (B, 256, K=32)\n",
        "\n",
        "        # 6.Fuse FSM output with decoder path\n",
        "        # Reshape FSM output `(B, 256, K=32)` into `(B, K, 16, 16)` for spatial compatibility\n",
        "        fsm_output_spatial_for_fusion = fsm_output_raw.permute(0, 2, 1).contiguous().view(\n",
        "            fsm_output_raw.size(0), self.fsm.K, 16, 16\n",
        "        )\n",
        "        #Project `K` channels to `2048` channels to match `x_bridge` for element-wise addition (fusion)\n",
        "        fsm_output_fused = self.fsm_output_fusion_transform(fsm_output_spatial_for_fusion)\n",
        "\n",
        "        #Fusion: Add FSM contribution to the bridge output (main decoder input)\n",
        "        fused_decoder_input = x_bridge + fsm_output_fused\n",
        "\n",
        "        #7.CNN Decoder: Reconstructs the mask from fused features\n",
        "        output = self.decoder(fused_decoder_input, pre_pools)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "#Dummy Data Testing\n",
        "#This block will test the model's forward pass and print its structure.\n",
        "if __name__ == '__main__':\n",
        "    print(\"Initializing TransCMFDBaseline model...\")\n",
        "    model = TransCMFDBaseline().cuda() # Instantiate model and move to GPU\n",
        "    print(\"TransCMFDBaseline model successfully loaded on cuda.\")\n",
        "\n",
        "    #Print the model summary to verify custom modules that I have used in this architecture\n",
        "    print(\"\\n--- Model Architecture Summary ---\")\n",
        "    print(model)\n",
        "    print(\"----------------------------------\\n\")\n",
        "\n",
        "    dummy_input = torch.rand((2, 3, 512, 512)).cuda() #Dummy input: Batch size 2, 3 channels (RGB), 512x512 resolution of image\n",
        "    print(f\"Dummy input shape: {dummy_input.shape}\")\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient computation for faster dummy pass (just for dummy input usage)\n",
        "        output = model(dummy_input)\n",
        "\n",
        "    print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHc_QIn9e6IS",
        "outputId": "c12b0015-3fea-4421-e564-24107d58e738"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing TransCMFDBaseline model...\n",
            "TransCMFDBaseline model successfully loaded on cuda.\n",
            "\n",
            "--- Model Architecture Summary ---\n",
            "TransCMFDBaseline(\n",
            "  (encoder): Encoder(\n",
            "    (input_block): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (down_blocks): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (bridge): Bridge(\n",
            "    (bridge): Sequential(\n",
            "      (0): ConvBlock(\n",
            "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): ConvBlock(\n",
            "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (adaptive_transformer_encoder): AdaptiveTransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x AdaptiveTransformerLayer(\n",
            "        (adaptive_mhsa): AdaptiveMultiHeadSelfAttention(\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (proj_to_dim): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (transformer_output_to_fsm_input): Upsample(size=(256, 256), mode='bilinear')\n",
            "  (fsm): FeatureSimilarityModule()\n",
            "  (fsm_output_fusion_transform): Conv2d(32, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (decoder): Decoder(\n",
            "    (up_block1): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(3072, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (up_block2): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (up_block3): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (up_block4): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (last_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (last_conv): ConvBlock(\n",
            "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (gn): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (out): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n",
            "----------------------------------\n",
            "\n",
            "Dummy input shape: torch.Size([2, 3, 512, 512])\n",
            "Output shape: torch.Size([2, 2, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class CMFDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Copy-Move Forgery Detection using CASIA v2.0.\n",
        "    Loads (Tampered Image, Ground Truth Mask) pairs and applies transformations.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_root, transform=None, img_size=(512, 512)):\n",
        "        self.data_root = data_root\n",
        "\n",
        "        self.tampered_folder = os.path.join(data_root, 'Tp')\n",
        "        self.gt_folder = os.path.join(data_root, 'CASIA 2 Groundtruth')\n",
        "\n",
        "        self.transform = transform\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.data_pairs = []\n",
        "\n",
        "        tampered_image_files = sorted([f for f in os.listdir(self.tampered_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "\n",
        "        for img_name in tampered_image_files:\n",
        "            img_path = os.path.join(self.tampered_folder, img_name)\n",
        "\n",
        "            # This part assumes mask names like 'Tp_D_CNN_S_N_txt00043_txt00051_10378_gt.png' from 'Tp_D_CNN_S_N_txt00043_txt00051_10378.jpg'\n",
        "\n",
        "            base_name_without_ext = os.path.splitext(img_name)[0]\n",
        "            mask_name = f\"{base_name_without_ext}_gt.png\"\n",
        "            mask_path = os.path.join(self.gt_folder, mask_name)\n",
        "\n",
        "            if os.path.exists(mask_path):\n",
        "                self.data_pairs.append((img_path, mask_path))\n",
        "            else:\n",
        "                print(f\"Warning: Mask not found for {img_name} at {mask_path}. Skipping.\")\n",
        "\n",
        "        print(f\"Loaded {len(self.data_pairs)} valid tampered image-mask pairs from {data_root}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.data_pairs[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        image = image.resize(self.img_size)\n",
        "        mask = mask.resize(self.img_size, Image.NEAREST)\n",
        "\n",
        "        image_tensor = transforms.ToTensor()(image)\n",
        "        mask_tensor = transforms.ToTensor()(mask)\n",
        "        mask_tensor = (mask_tensor > 0.5).float()\n",
        "\n",
        "        if self.transform:\n",
        "            image_tensor = self.transform(image_tensor)\n",
        "\n",
        "        return image_tensor, mask_tensor"
      ],
      "metadata": {
        "id": "tiNXmAAJ8o__"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define transformations (ImageNet normalization is standard for ResNet pretrained)\n",
        "transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "CASIA_V2_ROOT = '/content/drive/MyDrive/TransCMFD_dataset/CASIA2'\n",
        "\n",
        "# Create the dataset instance\n",
        "full_dataset = CMFDataset(\n",
        "    data_root=CASIA_V2_ROOT,\n",
        "    transform=transform,\n",
        "    img_size=(512, 512)\n",
        ")\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "# If CASIA v2.0 has ~5123 images, this split is reasonable.\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Dataset split: Training {len(train_dataset)} samples, Validation {len(val_dataset)} samples\")\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 8 # Keep small initially, adjust based on GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) # num_workers > 0 for faster loading\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train DataLoader batches: {len(train_loader)}\")\n",
        "print(f\"Validation DataLoader batches: {len(val_loader)}\")\n",
        "\n",
        "# --- Test Data Loading ---\n",
        "print(\"\\nTesting data loading...\")\n",
        "try:\n",
        "    # Fetch one batch to verify shapes and types\n",
        "    images, masks = next(iter(train_loader)) # Use next(iter()) to get one batch\n",
        "    print(f\"Batch images shape: {images.shape}\") # Should be (BATCH_SIZE, 3, 512, 512)\n",
        "    print(f\"Batch images dtype: {images.dtype}\") # Should be torch.float32\n",
        "    print(f\"Batch masks shape: {masks.shape}\")   # Should be (BATCH_SIZE, 1, 512, 512)\n",
        "    print(f\"Batch masks dtype: {masks.dtype}\")   # Should be torch.float32\n",
        "    print(\"Data loading successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data loading test: {e}\")\n",
        "    print(\"Please double-check your CASIA_V2_ROOT path, folder structure (Tampered, Groundtruth), and mask naming.\")\n",
        "    print(\"Common issues: wrong root path, incorrect subfolder names, mask files not found for image files.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kqilib75fqp",
        "outputId": "a4db366a-48e1-4a2c-8fae-c29bdc035451"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Mask not found for Tp_D_CNN_M_N_ind00091_ind00091_10647.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CNN_M_N_ind00091_ind00091_10647_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CNN_M_N_ind00091_ind00091_10648.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CNN_M_N_ind00091_ind00091_10648_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CNN_M_N_nat00077_nat00077_10574.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CNN_M_N_nat00077_nat00077_10574_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CNN_M_N_nat00089_nat00089_10576.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CNN_M_N_nat00089_nat00089_10576_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CNN_M_N_nat00090_nat00090_10575.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CNN_M_N_nat00090_nat00090_10575_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CNN_M_N_pla00042_pla00042_10976.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CNN_M_N_pla00042_pla00042_10976_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CNN_S_N_cha10122_nat00059_12169.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CNN_S_N_cha10122_nat00059_12169_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CNN_S_N_cha10122_nat10124_12167.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CNN_S_N_cha10122_nat10124_12167_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CNN_S_N_cha10122_nat10139_12166.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CNN_S_N_cha10122_nat10139_12166_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CRD_M_N_arc10116_arc10116_10762.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CRD_M_N_arc10116_arc10116_10762_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CRD_M_N_ind00091_ind00091_10645.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CRD_M_N_ind00091_ind00091_10645_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CRD_M_N_ind00091_ind00091_10646.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CRD_M_N_ind00091_ind00091_10646_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CRN_L_N_art00028_art00028_11282.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CRN_L_N_art00028_art00028_11282_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CRN_S_N_cha10122_nat10123_12168.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CRN_S_N_cha10122_nat10123_12168_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_CRN_S_N_pla00044_pla00044_10974.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CRN_S_N_pla00044_pla00044_10974_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NNN_S_N_nat00011_nat00011_11057.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NNN_S_N_nat00011_nat00011_11057_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NNN_S_N_nat00012_nat00012_11055.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NNN_S_N_nat00012_nat00012_11055_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NNN_S_N_pla00066_pla00066_11218.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NNN_S_N_pla00066_pla00066_11218_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRD_M_N_arc00004_nat00062_11177.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRD_M_N_arc00004_nat00062_11177_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRD_S_N_art00076_art00076_11709.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRD_S_N_art00076_art00076_11709_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRD_S_N_cha00015_cha00015_11154.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRD_S_N_cha00015_cha00015_11154_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRD_S_N_cha10002_cha10001_20094.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRD_S_N_cha10002_cha10001_20094_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_M_N_arc00004_nat00062_11176.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_M_N_arc00004_nat00062_11176_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_M_N_cha00033_cha00033_11015.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_M_N_cha00033_cha00033_11015_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_M_N_ind10103_cha10110_11553.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_M_N_ind10103_cha10110_11553_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_M_N_nat00089_pla10118_10583.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_M_N_nat00089_pla10118_10583_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_M_N_nat10145_nat10145_11978.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_M_N_nat10145_nat10145_11978_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_S_B_sec00029_nat00029_20113.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_S_B_sec00029_nat00029_20113_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_S_N_ani00054_ani00054_11129.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_S_N_ani00054_ani00054_11129_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_S_N_cha00086_cha00086_11403.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_S_N_cha00086_cha00086_11403_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_S_N_pla00030_pla00030_10951.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_S_N_pla00030_pla00030_10951_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_D_NRN_S_N_txt00029_txt00029_10832.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_NRN_S_N_txt00029_txt00029_10832_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CNN_S_N_cha10114_nat10114_12182.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CNN_S_N_cha10114_nat10114_12182_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_M_N_nat10164_nat00021_12098.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_M_N_nat10164_nat00021_12098_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_arc00004_arc00004_11175.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_arc00004_arc00004_11175_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_arc00062_ani00005_11499.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_arc00062_ani00005_11499_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_arc00078_art00092_11881.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_arc00078_art00092_11881_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_cha00021_cha00021_11182 (1).jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_cha00021_cha00021_11182 (1)_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_cha00038_cha00038_11031 (1).jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_cha00038_cha00038_11031 (1)_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_cha00040_cha00040_11032 (1).jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_cha00040_cha00040_11032 (1)_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_cha00040_cha00040_11034 (1).jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_cha00040_cha00040_11034 (1)_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_cha00045_cha00045_11007 (1).jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_cha00045_cha00045_11007 (1)_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_cha00053_cha00053_11720 (1).jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_cha00053_cha00053_11720 (1)_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_cha10113_cha10113_11543 (1).jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_cha10113_cha10113_11543 (1)_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_cha10135_cha10135_12206 (1).jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_cha10135_cha10135_12206 (1)_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_cha10206_cha10207_12346.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_cha10206_cha10207_12346_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_CRN_S_N_ind00046_ind00046_10892.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_CRN_S_N_ind00046_ind00046_10892_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NNN_M_N_nat10164_nat10160_12096.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NNN_M_N_nat10164_nat10160_12096_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NNN_M_N_sec00054_cha00062_11473.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NNN_M_N_sec00054_cha00062_11473_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NNN_M_O_nat00045_nat00099_10558.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NNN_M_O_nat00045_nat00099_10558_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NNN_S_B_pla00019_pla00020_20034.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NNN_S_B_pla00019_pla00020_20034_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NNN_S_N_ani00055_ani00051_11147.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NNN_S_N_ani00055_ani00051_11147_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NNN_S_N_cha00061_cha00043_11021.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NNN_S_N_cha00061_cha00043_11021_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NNN_S_N_sec00054_sec00055_11341.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NNN_S_N_sec00054_sec00055_11341_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NNN_S_O_txt00039_txt00042_10831.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NNN_S_O_txt00039_txt00042_10831_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_M_N_art10112_sec00098_11671.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_M_N_art10112_sec00098_11671_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_M_N_cha10114_nat10114_12181.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_M_N_cha10114_nat10114_12181_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_M_N_nat10164_nat00097_12097.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_M_N_nat10164_nat00097_12097_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_M_N_nat10164_nat10124_12095.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_M_N_nat10164_nat10124_12095_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_S_B_cha10125_pla00050_12163.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_S_B_cha10125_pla00050_12163_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_S_B_txt00022_txt00022_20090.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_S_B_txt00022_txt00022_20090_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_S_N_arc00062_ani00017_11500.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_S_N_arc00062_ani00017_11500_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_S_N_arc00062_ani00058_11497.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_S_N_arc00062_ani00058_11497_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_S_N_arc00062_ani00070_11498.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_S_N_arc00062_ani00070_11498_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_S_N_cha10167_cha10208_12368.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_S_N_cha10167_cha10208_12368_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_S_N_ind00046_ind00046_10893.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_S_N_ind00046_ind00046_10893_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_S_N_ind10103_ind10104_11552.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_S_N_ind10103_ind10104_11552_gt.png. Skipping.\n",
            "Warning: Mask not found for Tp_S_NRN_S_O_cha00035_cha00067_11734.jpg at /content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_S_NRN_S_O_cha00035_cha00067_11734_gt.png. Skipping.\n",
            "Loaded 2004 valid tampered image-mask pairs from /content/drive/MyDrive/TransCMFD_dataset/CASIA2\n",
            "Dataset split: Training 1603 samples, Validation 401 samples\n",
            "Train DataLoader batches: 201\n",
            "Validation DataLoader batches: 51\n",
            "\n",
            "Testing data loading...\n",
            "Batch images shape: torch.Size([8, 3, 512, 512])\n",
            "Batch images dtype: torch.float32\n",
            "Batch masks shape: torch.Size([8, 1, 512, 512])\n",
            "Batch masks dtype: torch.float32\n",
            "Data loading successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "CASIA_V2_ROOT = '/content/drive/MyDrive/TransCMFD_dataset/CASIA2'\n",
        "\n",
        "print(f\"Contents of {CASIA_V2_ROOT}:\")\n",
        "try:\n",
        "    for item in os.listdir(CASIA_V2_ROOT):\n",
        "        item_path = os.path.join(CASIA_V2_ROOT, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"  [DIR] {item}\")\n",
        "        else:\n",
        "            print(f\"  [FILE] {item}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {CASIA_V2_ROOT} not found. Please double-check the path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TbJN_T68fhJ",
        "outputId": "d8316e1c-faad-4428-d305-158b731f7e89"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of /content/drive/MyDrive/TransCMFD_dataset/CASIA2:\n",
            "  [DIR] CASIA 2 Groundtruth\n",
            "  [DIR] Tp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "CASIA_V2_ROOT = '/content/drive/MyDrive/TransCMFD_dataset/CASIA2'\n",
        "TAMPERED_FOLDER = os.path.join(CASIA_V2_ROOT, 'Tp')\n",
        "GT_FOLDER = os.path.join(CASIA_V2_ROOT, 'CASIA 2 Groundtruth')\n",
        "\n",
        "print(f\"--- Listing Tampered Images in '{TAMPERED_FOLDER}' ---\")\n",
        "try:\n",
        "    tampered_files = sorted([f for f in os.listdir(TAMPERED_FOLDER) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "    if tampered_files:\n",
        "        print(f\"Found {len(tampered_files)} tampered images. First 5:\")\n",
        "        for i, fname in enumerate(tampered_files[:5]):\n",
        "            print(f\"  {fname}\")\n",
        "\n",
        "        if tampered_files:\n",
        "            sample_img_name = tampered_files[0]\n",
        "            base_name_without_ext = os.path.splitext(sample_img_name)[0]\n",
        "            derived_mask_name = f\"{base_name_without_ext}_gt.png\" # This is the current assumption\n",
        "            expected_mask_path = os.path.join(GT_FOLDER, derived_mask_name)\n",
        "\n",
        "            print(f\"\\n--- Checking Mask for Sample Image: '{sample_img_name}' ---\")\n",
        "            print(f\"  Derived mask name: '{derived_mask_name}'\")\n",
        "            print(f\"  Expected mask path: '{expected_mask_path}'\")\n",
        "\n",
        "            if os.path.exists(expected_mask_path):\n",
        "                print(\"  --> Mask file EXISTS at the derived path! This is good.\")\n",
        "            else:\n",
        "                print(\"  --> Mask file DOES NOT EXIST at the derived path! This is the problem.\")\n",
        "                print(\"  Please check the exact naming convention of your mask files in the 'CASIA 2 Groundtruth' folder.\")\n",
        "                print(\"  Look for a mask file that corresponds to this image:\")\n",
        "                print(f\"    Image: {sample_img_name}\")\n",
        "                print(\"  And provide its exact mask filename.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"No image files found in '{TAMPERED_FOLDER}'. Please check the path and content.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{TAMPERED_FOLDER}' not found. Check subfolder name or root path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axAsCyvv9RNy",
        "outputId": "e47da6c0-4870-4596-8c2c-23840deadc9b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Listing Tampered Images in '/content/drive/MyDrive/TransCMFD_dataset/CASIA2/Tp' ---\n",
            "Found 2072 tampered images. First 5:\n",
            "  Tp_D_CND_S_N_txt00028_txt00006_10848.jpg\n",
            "  Tp_D_CNN_M_B_nat00056_nat00099_11105.jpg\n",
            "  Tp_D_CNN_M_B_nat10139_nat00059_11949.jpg\n",
            "  Tp_D_CNN_M_B_nat10139_nat00097_11948.jpg\n",
            "  Tp_D_CNN_M_N_ani00052_ani00054_11130.jpg\n",
            "\n",
            "--- Checking Mask for Sample Image: 'Tp_D_CND_S_N_txt00028_txt00006_10848.jpg' ---\n",
            "  Derived mask name: 'Tp_D_CND_S_N_txt00028_txt00006_10848_gt.png'\n",
            "  Expected mask path: '/content/drive/MyDrive/TransCMFD_dataset/CASIA2/CASIA 2 Groundtruth/Tp_D_CND_S_N_txt00028_txt00006_10848_gt.png'\n",
            "  --> Mask file EXISTS at the derived path! This is good.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "    def forward(self, prediction, target):\n",
        "        target = target.float()\n",
        "        prediction = torch.sigmoid(prediction)\n",
        "        prediction_flat = prediction.contiguous().view(-1)\n",
        "        target_flat = target.contiguous().view(-1)\n",
        "        intersection = (prediction_flat * target_flat).sum()\n",
        "        dice_coefficient = (2. * intersection + self.smooth) / (prediction_flat.sum() + target_flat.sum() + self.smooth)\n",
        "        return 1 - dice_coefficient\n",
        "\n",
        "class AdaptiveRegularizationLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdaptiveRegularizationLoss, self).__init__()\n",
        "    def forward(self, model):\n",
        "        l_adapt_total = 0.0\n",
        "        if hasattr(model, 'adaptive_transformer_encoder') and hasattr(model.adaptive_transformer_encoder, 'layers'):\n",
        "            for layer in model.adaptive_transformer_encoder.layers:\n",
        "                if hasattr(layer, 'adaptive_mhsa'):\n",
        "                    s1_param = layer.adaptive_mhsa.S1\n",
        "                    s2_param = layer.adaptive_mhsa.S2\n",
        "                    l_adapt_total += torch.norm(s1_param, 2)**2\n",
        "                    l_adapt_total += torch.norm(s2_param, 2)**2\n",
        "        return l_adapt_total\n",
        "\n",
        "\n",
        "\n",
        "# Define Loss Functions\n",
        "dice_loss_fn = DiceLoss(smooth=1.0)\n",
        "bce_loss_fn = nn.BCEWithLogitsLoss()\n",
        "adaptive_reg_loss_fn = AdaptiveRegularizationLoss()\n",
        "\n",
        "# Define Optimizer\n",
        "LEARNING_RATE = 1e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Loss weighting factors\n",
        "ALPHA = 0.5\n",
        "BETA = 0.5\n",
        "GAMMA = 0.01\n",
        "\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, dice_loss, bce_loss, adaptive_reg_loss, num_epochs, alpha, beta, gamma):\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Initialize GradScaler for Automatic Mixed Precision (AMP)\n",
        "    scaler = torch.cuda.amp.GradScaler() #\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Clear CUDA cache before training loop to free up any residual memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_loop = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(train_loop):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Use autocast for mixed precision training\n",
        "            with torch.cuda.amp.autocast():\n",
        "                predictions = model(images)\n",
        "                l_dice = dice_loss(predictions, masks)\n",
        "                l_bce = bce_loss(predictions, masks)\n",
        "                l_adapt = adaptive_reg_loss(model)\n",
        "\n",
        "                total_loss = alpha * l_dice + beta * l_bce + gamma * l_adapt\n",
        "\n",
        "            # Scale the loss and perform backward pass\n",
        "            scaler.scale(total_loss).backward()\n",
        "\n",
        "            # Optimizer step\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Update the scaler for next iteration\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += total_loss.item() * images.size(0)\n",
        "\n",
        "            train_loop.set_postfix(loss=total_loss.item())\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_loop = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\")\n",
        "            for images, masks in val_loop:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "\n",
        "                # Use autocast for validation too (no scaler.scale/update needed)\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    predictions = model(images)\n",
        "                    l_dice_val = dice_loss(predictions, masks)\n",
        "                    l_bce_val = bce_loss(predictions, masks)\n",
        "                    l_adapt_val = adaptive_reg_loss(model)\n",
        "                    total_val_loss = alpha * l_dice_val + beta * l_bce_val + gamma * l_adapt_val\n",
        "\n",
        "                val_loss += total_val_loss.item() * images.size(0)\n",
        "                val_loop.set_postfix(val_loss=total_val_loss.item())\n",
        "\n",
        "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1} Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            checkpoint_path = '/content/drive/MyDrive/TransCMFD_Checkpoints/best_model.pth'\n",
        "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Model saved to {checkpoint_path} with validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    BATCH_SIZE = 1\n",
        "    NUM_WORKERS = 0\n",
        "\n",
        "    print(f\"Using BATCH_SIZE: {BATCH_SIZE}, NUM_WORKERS: {NUM_WORKERS}\")\n",
        "\n",
        "\n",
        "    try:\n",
        "\n",
        "        train_size = int(0.8 * len(full_dataset))\n",
        "        val_size = len(full_dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "        print(\"DataLoaders re-created with new batch size and num_workers.\")\n",
        "    except NameError:\n",
        "        print(\"Warning: full_dataset not found. Ensure CMFDataset and DataLoader setup cells were run correctly.\")\n",
        "        print(\"Please define/re-run your data loading setup to ensure train_loader and val_loader are available.\")\n",
        "\n",
        "\n",
        "    print(\"\\nStarting model training...\")\n",
        "    train_model(model, train_loader, val_loader, optimizer, dice_loss_fn, bce_loss_fn, adaptive_reg_loss_fn, NUM_EPOCHS, ALPHA, BETA, GAMMA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621,
          "referenced_widgets": [
            "5055464f662c4527a9d94293c51f4163",
            "e609002a4e274ea5a7e82b5ce6e56041",
            "ef3198b99c194ae48c2f41e8f0812bd5",
            "6a29e2017fb74f9eb7504f35480e2994",
            "a0f1709b88e84eb3aefda4daa986bb04",
            "1193b765558d4488aca27255ca4c4a7b",
            "d91b291aa71949028e4f6cc1a569bccc",
            "312a28d53df74d5089b5f1fdab09d69e",
            "4bd13eb4afdc4d5eba08e17184efd155",
            "da34faa103124a01bf667fcae0352d36",
            "689bc355db89414ebbdd76c959a43e43"
          ]
        },
        "id": "BCZ9By7f9Kyv",
        "outputId": "abf2ef4a-e7e2-4b01-bef1-a3ae2951420b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using BATCH_SIZE: 1, NUM_WORKERS: 0\n",
            "DataLoaders re-created with new batch size and num_workers.\n",
            "\n",
            "Starting model training...\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-42-2526065849.py:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() #\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training Epoch 1:   0%|          | 0/1603 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5055464f662c4527a9d94293c51f4163"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-42-2526065849.py:77: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmStridedBatchedEx( handle, opa, opb, m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, (void*)(&fbeta), c, CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-42-2526065849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting model training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdice_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madaptive_reg_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALPHA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBETA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-42-2526065849.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, dice_loss, bce_loss, adaptive_reg_loss, num_epochs, alpha, beta, gamma)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Use autocast for mixed precision training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0ml_dice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0ml_bce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-37-1795299283.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;31m# 3.Adaptive Transformer Encoder: Learns global representations from tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mtransformer_output_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_transformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output_c4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Output: (B, 256, dim=512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;31m# 4.Transform Transformer Output to FSM Input (\"image-like feature F\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-37-1795299283.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feature_map_c4)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m#Pass through the stack of transformer layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;31m#Output tokens: (batch_size, sequence_length, embedding_dimension)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-37-1795299283.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m#x is expected as token sequence: (batch_size, sequence_length, embedding_dimension)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_mhsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Custom MHSA call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Add residual connection and apply LayerNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-37-1795299283.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m#Compute Attention Weights (QK^T / sqrt(d_k))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;31m#Apply Softmax to get probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmStridedBatchedEx( handle, opa, opb, m, n, k, (void*)(&falpha), a, CUDA_R_16F, lda, stridea, b, CUDA_R_16F, ldb, strideb, (void*)(&fbeta), c, CUDA_R_16F, ldc, stridec, num_batches, CUDA_R_32F, CUBLAS_GEMM_DEFAULT_TENSOR_OP)`"
          ]
        }
      ]
    }
  ]
}
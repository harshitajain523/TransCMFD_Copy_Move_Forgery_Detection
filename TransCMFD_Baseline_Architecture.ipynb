{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMFzceAFB7IQ/wcSUjE5HRg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitajain523/TransCMFD_Copy_Move_Forgery_Detection/blob/main/TransCMFD_Baseline_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n",
        "#mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "m7lc2v1yW1OG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9b82e1-4230-4d41-cbff-a66f5538c1aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "CUDA device name: NVIDIA A100-SXM4-40GB\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#  Helper Modules\n",
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, padding=1, kernel_size=3, stride=1, with_nonlinearity=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, padding=padding, kernel_size=kernel_size, stride=stride)\n",
        "        self.gn = nn.GroupNorm(32, out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.with_nonlinearity = with_nonlinearity\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.gn(x)\n",
        "        if self.with_nonlinearity:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "class Bridge(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.bridge = nn.Sequential(\n",
        "            ConvBlock(in_channels, out_channels),\n",
        "            ConvBlock(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bridge(x)\n",
        "\n",
        "# ----------------------------------------------------------------------------- #\n",
        "#upblock\n",
        "class UpBlockForUNetWithResNet50(nn.Module):\n",
        "    #Consists of Upsample ->(Concatenation with skip connection)->ConvBlock.\n",
        "    def __init__(self, in_channels_after_concat, out_channels, up_conv_in_channels, up_conv_out_channels,\n",
        "                 upsampling_method=\"bilinear\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsampling_method = upsampling_method\n",
        "        if upsampling_method == \"conv_transpose\":\n",
        "            self.upsample_layer = nn.ConvTranspose2d(up_conv_in_channels, up_conv_out_channels, kernel_size=2, stride=2)\n",
        "        elif upsampling_method == \"bilinear\":\n",
        "            self.upsample_layer = nn.Upsample(scale_factor=2.0, mode='bilinear', align_corners=False)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported upsampling_method\")\n",
        "\n",
        "        self.conv_block = ConvBlock(in_channels_after_concat, out_channels)\n",
        "\n",
        "    def forward(self, up_x, down_x):\n",
        "        up_x = self.upsample_layer(up_x)\n",
        "\n",
        "        if up_x.shape[2] != down_x.shape[2] or up_x.shape[3] != down_x.shape[3]:\n",
        "            up_x = F.interpolate(up_x, size=(down_x.shape[2], down_x.shape[3]), mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = torch.cat([up_x, down_x], 1)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------- #\n",
        "\n",
        "#CNN Encoder\n",
        "class Encoder(nn.Module):\n",
        "    #CNN Encoder based on a pre-trained ResNet-50, using the channels for resnet 50 itself.\n",
        "    DEPTH = 6\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        resnet = torchvision.models.resnet.resnet50(pretrained=True)##using pretrained weights\n",
        "\n",
        "        self.input_block = nn.Sequential(*list(resnet.children()))[:4]\n",
        "\n",
        "        down_blocks = []\n",
        "        for bottleneck_stage in list(resnet.children())[4:]:\n",
        "            if isinstance(bottleneck_stage, nn.Sequential):\n",
        "                down_blocks.append(bottleneck_stage)\n",
        "        self.down_blocks = nn.ModuleList(down_blocks)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre_pools = dict()\n",
        "        pre_pools[\"layer_0\"] = x #original input image for the very last decoder stage\n",
        "\n",
        "        x = self.input_block(x)\n",
        "        pre_pools[\"layer_1\"] = x\n",
        "\n",
        "        for i, block in enumerate(self.down_blocks):\n",
        "            x = block(x)\n",
        "            if i < len(self.down_blocks) - 1: #ALL but the last one (layer4 output is going to bridge)\n",
        "                pre_pools[f\"layer_{i+2}\"] = x\n",
        "\n",
        "        return x, pre_pools\n",
        "# ----------------------------------------------------------------------------- #\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        # Decoder stages: in_channels_after_concat refers to channels after upsample + skip concat.\n",
        "        # Channels adapted for ResNet50 outputs.\n",
        "        # For 512x512 input, spatial resolutions are:\n",
        "        # Encoder: Input -> 128x128 (layer1, input_block) -> 64x64 (layer2) -> 32x32 (layer3) -> 16x16 (layer4/bridge)\n",
        "        # Decoder stages will upsample: 16x16 -> 32x32 -> 64x64 -> 128x128 -> 256x256 -> 512x512\n",
        "\n",
        "        #Up-block 1: Fuses deepest feature (from bridge/transformer) with ResNet.layer3 output\n",
        "        # up_x (from bridge) is 2048 channels, 16x16. Skip (pre_pools[\"layer_4\"]) is 1024 channels, 32x32.\n",
        "        # Concatenated: 2048 + 1024 = 3072 channels. Output: 1024 channels, 32x32 spatial.\n",
        "        self.up_block1 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=2048 + 1024, out_channels=1024,\n",
        "            up_conv_in_channels=2048, up_conv_out_channels=2048\n",
        "        )\n",
        "        # Up-block 2: Fuses output of up_block1 with ResNet.layer2 output\n",
        "        # up_x (from up_block1) is 1024 channels, 32x32. Skip (pre_pools[\"layer_3\"]) is 512 channels, 64x64.\n",
        "        # Concatenated: 1024 + 512 = 1536 channels. Output: 512 channels, 64x64 spatial.\n",
        "        self.up_block2 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=1024 + 512, out_channels=512,\n",
        "            up_conv_in_channels=1024, up_conv_out_channels=1024\n",
        "        )\n",
        "        # Up-block 3: Fuses output of up_block2 with ResNet.layer1 output\n",
        "        # up_x (from up_block2) is 512 channels, 64x64. Skip (pre_pools[\"layer_2\"]) is 256 channels, 128x128.\n",
        "        # Concatenated: 512 + 256 = 768 channels. Output: 256 channels, 128x128 spatial.\n",
        "        self.up_block3 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=512 + 256, out_channels=256,\n",
        "            up_conv_in_channels=512, up_conv_out_channels=512\n",
        "        )\n",
        "        # Up-block 4: Fuses output of up_block3 with Encoder's input_block output (after manual upsampling)\n",
        "        # up_x (from up_block3) is 256 channels, 128x128. Skip (pre_pools[\"layer_1\"]) is 64 channels, 128x128.\n",
        "        # After explicit upsampling of `pre_pools[\"layer_1\"]` to 256x256 in forward,\n",
        "        # Concatenated: 256 + 64 = 320 channels. Output: 128 channels, 256x256 spatial.\n",
        "        self.up_block4 = UpBlockForUNetWithResNet50(\n",
        "            in_channels_after_concat=256 + 64, out_channels=128,\n",
        "            up_conv_in_channels=256, up_conv_out_channels=256\n",
        "        )\n",
        "\n",
        "        # Final upsampling and convolution as per model summary\n",
        "        self.last_upsample = nn.Upsample(scale_factor=2.0, mode='bilinear', align_corners=False)\n",
        "        self.last_conv = ConvBlock(in_channels=128, out_channels=64, with_nonlinearity=True)\n",
        "\n",
        "        # Output layer to produce the binary tampering mask\n",
        "        self.out = nn.Conv2d(64, n_classes, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, pre_pools):\n",
        "        # x is the deepest feature (from bridge/transformer path), 16x16 spatial\n",
        "        x = self.up_block1(x, pre_pools[\"layer_4\"]) # Output: 1024 channels, 32x32 spatial\n",
        "        x = self.up_block2(x, pre_pools[\"layer_3\"]) # Output: 512 channels, 64x64 spatial\n",
        "        x = self.up_block3(x, pre_pools[\"layer_2\"]) # Output: 256 channels, 128x128 spatial\n",
        "\n",
        "        #############CRITICAL FIX#####: Explicitly upsample pre_pools[\"layer_1\"] to 256x256 for up_block4\n",
        "        #this ensures spatial compatibility for concatenation with up_x (which becomes 256x256 after upsampling in UpBlock).\n",
        "        upsampled_skip_layer1 = F.interpolate(\n",
        "            pre_pools[\"layer_1\"],\n",
        "            size=(256, 256), #Target size for this skip connection\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        )\n",
        "        x = self.up_block4(x, upsampled_skip_layer1) # \\Output: 128 channels, 256x256 spatial.\n",
        "\n",
        "        # Final upsampling and convolution to reach original input resolution (512x512)\n",
        "        x = self.last_upsample(x) # Upsamples 256x256 to 512x512\n",
        "        x = self.last_conv(x)      # Applies ConvBlock (128 channels -> 64 channels, 512x512)\n",
        "\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# ----------------------------------------------------------------------------- #\n",
        "class FeatureSimilarityModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature Similarity Module (FSM) implementing block-wise Pearson correlation and percentile pooling.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(FeatureSimilarityModule, self).__init__()\n",
        "        self.K = 32 # Number of top similarity scores to select per block\n",
        "\n",
        "    def _pearson_correlation_coefficient(self, B_i, B_j):\n",
        "        \"\"\"\n",
        "        Calculates the Pearson correlation coefficient between two flattened feature blocks.\n",
        "        \"\"\"\n",
        "        B_i = B_i.float()\n",
        "        B_j = B_j.float()\n",
        "\n",
        "        mean_B_i = torch.mean(B_i)\n",
        "        mean_B_j = torch.mean(B_j)\n",
        "\n",
        "        std_B_i = torch.std(B_i, unbiased=False)\n",
        "        std_B_j = torch.std(B_j, unbiased=False)\n",
        "\n",
        "        if std_B_i == 0 or std_B_j == 0:\n",
        "            return torch.tensor(0.0, device=B_i.device)\n",
        "\n",
        "        normalized_B_i = (B_i - mean_B_i) / std_B_i\n",
        "        normalized_B_j = (B_j - mean_B_j) / std_B_j\n",
        "\n",
        "        correlation = torch.dot(normalized_B_i, normalized_B_j) / B_i.numel()\n",
        "        return correlation\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # Expected input `feature_map` is the \"image-like feature F\" from transformer output,\n",
        "        # anticipated to be `(batch_size, channels=512, H=256, W=256)`.\n",
        "        batch_size, channels, H, W = feature_map.shape\n",
        "\n",
        "        block_size_spatial = 16 #Original Paper specifies 16x16 non-overlapping blocks\n",
        "        num_blocks_h = H // block_size_spatial\n",
        "        num_blocks_w = W // block_size_spatial\n",
        "        num_blocks = num_blocks_h * num_blocks_w #Results in 256 total blocks for 256x256 input\n",
        "\n",
        "        #Self-Correlation Calculation Block: For feature map into blocks\n",
        "        blocks_unfolded = feature_map.unfold(2, block_size_spatial, block_size_spatial).unfold(3, block_size_spatial, block_size_spatial)\n",
        "\n",
        "        #Flatten each block into a 1D feature vector for similarity computation\n",
        "        blocks_flat = blocks_unfolded.permute(0, 2, 3, 1, 4, 5).contiguous().view(\n",
        "            batch_size, num_blocks, -1\n",
        "        )\n",
        "\n",
        "        #Compute pairwise Pearson correlation coefficients, forming a similarity matrix\n",
        "        similarity_matrix = torch.zeros(batch_size, num_blocks, num_blocks, device=feature_map.device)\n",
        "        for b in range(batch_size):\n",
        "            for i in range(num_blocks):\n",
        "                for j in range(num_blocks):\n",
        "                    similarity_matrix[b, i, j] = self._pearson_correlation_coefficient(blocks_flat[b, i], blocks_flat[b, j])\n",
        "\n",
        "        #Percentile Pooling Block: Select top K similarities\n",
        "        percentile_scores = torch.zeros(batch_size, num_blocks, self.K, device=feature_map.device)\n",
        "        for b in range(batch_size):\n",
        "            for i in range(num_blocks):\n",
        "                sorted_scores = torch.sort(similarity_matrix[b, i], descending=True).values\n",
        "\n",
        "                if self.K <= sorted_scores.shape[0]:\n",
        "                    percentile_scores[b, i] = sorted_scores[:self.K]\n",
        "                else:\n",
        "                    percentile_scores[b, i, :sorted_scores.shape[0]] = sorted_scores\n",
        "\n",
        "        return percentile_scores ####Output shape: (batch_size, 256, self.K)\n",
        "\n",
        "#Adaptive Transformer Components\n",
        "# Custom Adaptive Multi-Head Self-Attention for the transformer's core innovation as given in the paper\n",
        "class AdaptiveMultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom Adaptive Multi-Head Self-Attention (AdaptiveMSA) module.\n",
        "    Implements the Dual-Path Adaptive Attention Mechanism (DPAAM) as described in Algorithm 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads # d_k = d_v = d_model / h\n",
        "\n",
        "        if self.head_dim * num_heads != self.embed_dim:\n",
        "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads})\")\n",
        "\n",
        "        #Linear transformations for Query, Key, Value\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        #Learnable parameters for DPAAM: S1, S2, b1, b2, epsilon, delta\n",
        "        self.S1 = nn.Parameter(torch.ones(embed_dim)) #Initialized to 1 for identity-like start\n",
        "        self.S2 = nn.Parameter(torch.ones(embed_dim)) #Applied element-wise across the embedding dimension\n",
        "\n",
        "        self.b1 = nn.Parameter(torch.zeros(embed_dim)) #Learnable bias vectors\n",
        "        self.b2 = nn.Parameter(torch.zeros(embed_dim))\n",
        "\n",
        "        self.epsilon = nn.Parameter(torch.tensor(0.5)) #Weighting factors (initially balanced)\n",
        "        self.delta = nn.Parameter(torch.tensor(0.5))\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        batch_size, seq_len, embed_dim = query.size()\n",
        "\n",
        "        #Project Query, Key, Value\n",
        "        Q = self.q_proj(query)\n",
        "        K = self.k_proj(key)\n",
        "        V = self.v_proj(value)\n",
        "\n",
        "        #Reshape for multi-head attention: (B, SeqLen, EmbedDim) -> (B, NumHeads, SeqLen, HeadDim)\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        #Compute Attention Weights (QK^T / sqrt(d_k))\n",
        "        attention_weights = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        # --- DEBUGGING LINES START ---\n",
        "        if torch.isnan(attention_weights).any():\n",
        "            print(\"DEBUG: NaN found in attention_weights before softmax!\")\n",
        "            # You might want to print mean, max, min for Q, K, attention_weights\n",
        "            # print(f\"Q stats: min={Q.min().item():.2e}, max={Q.max().item():.2e}, mean={Q.mean().item():.2e}\")\n",
        "            # print(f\"K stats: min={K.min().item():.2e}, max={K.max().item():.2e}, mean={K.mean().item():.2e}\")\n",
        "            # print(f\"Attention_weights stats: min={attention_weights.min().item():.2e}, max={attention_weights.max().item():.2e}, mean={attention_weights.mean().item():.2e}\")\n",
        "            # Consider adding a small epsilon to the denominator of sqrt(d_k) if that's where NaNs come from\n",
        "            # Or clamping attention_weights if values are too extreme\n",
        "        if torch.isinf(attention_weights).any():\n",
        "            print(\"DEBUG: Inf found in attention_weights before softmax!\")\n",
        "        # --- DEBUGGING LINES END ---\n",
        "\n",
        "        #Apply Softmax to get probabilities\n",
        "        attention_probs = F.softmax(attention_weights, dim=-1)\n",
        "\n",
        "        #Calculate Head_i' (standard attention output before adaptivity)\n",
        "        Head_prime = torch.matmul(attention_probs, V)\n",
        "\n",
        "        #Reshape Head_prime back to (batch_size, seq_len, embed_dim) for element-wise DPAAM ops\n",
        "        Head_prime_combined = Head_prime.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        #Apply DPAAM (Dual-Path Adaptive Attention Mechanism)\n",
        "        M1_Head = self.S1 * Head_prime_combined + self.b1 # Element-wise scale and bias\n",
        "        M2_Head = self.S2 * Head_prime_combined + self.b2 # Element-wise scale and bias\n",
        "\n",
        "        #Apply sigmoid to epsilon and delta to constrain them to (0,1) as per paper\n",
        "        epsilon_val = torch.sigmoid(self.epsilon)\n",
        "        delta_val = torch.sigmoid(self.delta)\n",
        "\n",
        "        #Combine the mapped heads with epsilon and delta weights\n",
        "        adaptive_heads_combined = epsilon_val * M1_Head + delta_val * M2_Head\n",
        "\n",
        "        #Final output projection (W_0)\n",
        "        output = self.out_proj(adaptive_heads_combined)\n",
        "        return output\n",
        "\n",
        "#AdaptiveTransformerLayer (UPDATED to use custom AdaptiveMSA)\n",
        "class AdaptiveTransformerLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A single layer of the Adaptive Transformer Encoder, incorporating AdaptiveMSA and MLP.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super().__init__()\n",
        "        #Using the custom AdaptiveMultiHeadSelfAttention module\n",
        "        self.adaptive_mhsa = AdaptiveMultiHeadSelfAttention(embed_dim=dim, num_heads=num_heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim) #layer Normalization\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.mlp = nn.Sequential( #MLP (Feedforward Network)\n",
        "            nn.Linear(dim, dim * 4), #Common expansion factor of 4\n",
        "            nn.GELU(), #GELU activation is common in modern transformers\n",
        "            nn.Linear(dim * 4, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x is expected as token sequence: (batch_size, sequence_length, embedding_dimension)\n",
        "        attn_output = self.adaptive_mhsa(x, x, x) #Custom MHSA call\n",
        "        x = self.norm1(x + attn_output) #Add residual connection and apply LayerNorm\n",
        "\n",
        "        x_mlp = self.mlp(x)\n",
        "        x = self.norm2(x + x_mlp) #Add residual connection and apply LayerNorm\n",
        "        return x\n",
        "\n",
        "#AdaptiveTransformerEncoder (Uses AdaptiveTransformerLayer)\n",
        "'''class AdaptiveTransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The Adaptive Transformer Encoder, stacking multiple AdaptiveTransformerLayers.\n",
        "    Handles Tokenization and Positional Embedding for CNN features.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.dim = dim #Transformer's embedding dimension\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.layers = nn.ModuleList([AdaptiveTransformerLayer(dim, num_heads) for _ in range(num_layers)])\n",
        "\n",
        "        #Tokenization: Projects CNN encoder's C4 output channels (2048) to transformer's `dim`\n",
        "        self.proj_to_dim = nn.Conv2d(2048, dim, kernel_size=1)\n",
        "\n",
        "        #Learnable Positional Embedding\n",
        "        #Assuming fixed 16x16 spatial size for tokens (256 tokens) from a 512x512 input.\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 16 * 16, dim))\n",
        "\n",
        "    def forward(self, feature_map_c4):\n",
        "        # `feature_map_c4` is the deepest feature from the CNN Encoder (Bridge input):\n",
        "        #Expected shape: (batch_size, 2048, H_spatial, W_spatial) (e.g., 16x16 for 512x512 input)\n",
        "\n",
        "        x = self.proj_to_dim(feature_map_c4) # Output: (B, dim, H_spatial, W_spatial)\n",
        "        x = x.flatten(2).permute(0, 2, 1) # (B, H_spatial * W_spatial, dim) -> e.g., (B, 256, dim)\n",
        "\n",
        "        if x.shape[1] != self.pos_embedding.shape[1]:\n",
        "            raise ValueError(f\"Positional embedding sequence length mismatch. Expected {self.pos_embedding.shape[1]}, got {x.shape[1]}\")\n",
        "\n",
        "        x = x + self.pos_embedding #Add learnable positional encoding to tokens\n",
        "\n",
        "        #Pass through the stack of transformer layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x #Output tokens: (batch_size, sequence_length, embedding_dimension)'''\n",
        "class AdaptiveTransformerEncoder(nn.Module):\n",
        "    def __init__(self, dim, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.num_layers = num_layers\n",
        "        self.layers = nn.ModuleList([AdaptiveTransformerLayer(dim, num_heads) for _ in range(num_layers)])\n",
        "\n",
        "        self.proj_to_dim = nn.Conv2d(2048, dim, kernel_size=1)\n",
        "\n",
        "        # --- CRITICAL CHANGE: Adjust pos_embedding for 256x256 input ---\n",
        "        # For 256x256 input, the spatial size of the feature map before transformer is 8x8 (256 / 32)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, 8 * 8, dim)) # 8x8 = 64 tokens\n",
        "        # --- END CRITICAL CHANGE ---\n",
        "\n",
        "    def forward(self, feature_map_c4):\n",
        "        x = self.proj_to_dim(feature_map_c4)\n",
        "\n",
        "        # After proj_to_dim, x is (B, dim, 8, 8) if input was 256x256.\n",
        "        # Flatten spatial dimensions to 64 tokens.\n",
        "        x = x.flatten(2).permute(0, 2, 1) # (B, 64, dim)\n",
        "\n",
        "        if x.shape[1] != self.pos_embedding.shape[1]:\n",
        "            # This check will now specifically look for 64 tokens.\n",
        "            raise ValueError(f\"Positional embedding sequence length mismatch. Expected {self.pos_embedding.shape[1]}, got {x.shape[1]}\")\n",
        "\n",
        "        x = x + self.pos_embedding\n",
        "        for layer in self.layers: x = layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "#Main TransCMFDBaseline Model\n",
        "\n",
        "class TransCMFDBaseline(nn.Module):\n",
        "    \"\"\"\n",
        "    The complete TransCMFD model architecture.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.bridge = Bridge(2048, 2048)\n",
        "\n",
        "        #Adaptive Transformer Encoder setup\n",
        "        transformer_dim = 128\n",
        "        transformer_heads = 4\n",
        "        transformer_layers = 2 #Reduced to 2 layers as per plan for time constraint\n",
        "\n",
        "        self.adaptive_transformer_encoder = AdaptiveTransformerEncoder(\n",
        "            dim=transformer_dim,\n",
        "            num_heads=transformer_heads,\n",
        "            num_layers=transformer_layers\n",
        "        )\n",
        "\n",
        "        #Transforms Transformer Output (tokens) to FSM Input (spatial feature map F)\n",
        "        #Transformer output: (B, seq_len=256, dim=512)\n",
        "        #FSM expects input `F` as: (B, channels=512, H=256, W=256)\n",
        "\n",
        "        self.transformer_output_to_fsm_input = nn.Upsample(size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "        self.fsm = FeatureSimilarityModule() # Feature Similarity Module\n",
        "\n",
        "        #Transforms FSM output back to a spatial feature map for fusion with decoder path\n",
        "        #FSM output: (B, num_blocks=256, K=32)\n",
        "        #This layer takes `(B, K, 16, 16)` (after internal reshape) and projects channels `K` to `2048`\n",
        "        #to match `x_bridge` for fusion.\n",
        "        self.fsm_output_fusion_transform = nn.Conv2d(self.fsm.K, 2048, kernel_size=1)\n",
        "\n",
        "        self.decoder = Decoder(n_classes=n_classes) # CNN Decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1.CNN Encoder: Extracts local features and provides skip connections\n",
        "        encoder_output_c4, pre_pools = self.encoder(x)\n",
        "\n",
        "        # 2.Bridge: Connects encoder's deepest feature to transformer/decoder path\n",
        "        x_bridge = self.bridge(encoder_output_c4) # (B, 2048, 16, 16) for 512x512 input\n",
        "\n",
        "        # 3.Adaptive Transformer Encoder: Learns global representations from tokens\n",
        "        transformer_output_tokens = self.adaptive_transformer_encoder(encoder_output_c4) # Output: (B, 256, dim=512)\n",
        "\n",
        "        # 4.Transform Transformer Output to FSM Input (\"image-like feature F\")\n",
        "        # Reshape transformer tokens `(B, 256, 512)` into a spatial feature map `(B, 512, 16, 16)`\n",
        "        fsm_input_spatial_reshaped = transformer_output_tokens.permute(0, 2, 1).contiguous().view(\n",
        "            transformer_output_tokens.size(0),\n",
        "            self.adaptive_transformer_encoder.dim, # Channels (512)\n",
        "            8, 8 # <--- CRITICAL CHANGE: Spatial dimensions, derived from 64 tokens (8*8)\n",
        "        )\n",
        "        #Upsample this `(B, 512, 8, 8)` to `(B, 512, 256, 256)` as FSM expects\n",
        "        fsm_input = self.transformer_output_to_fsm_input(fsm_input_spatial_reshaped)\n",
        "\n",
        "        # 5.Feature Similarity Module (FSM): Identifies similar regions\n",
        "        fsm_output_raw = self.fsm(fsm_input) # Output: (B, 256, K=32)\n",
        "# Inside TransCMFDBaseline's forward method\n",
        "# 6. Fuse FSM output with decoder path\n",
        "# Reshape FSM output `(B, 64, K=32)` into `(B, K, 8, 8)` for spatial compatibility\n",
        "        fsm_output_spatial_for_fusion = fsm_output_raw.permute(0, 2, 1).contiguous().view(\n",
        "        fsm_output_raw.size(0), self.fsm.K,\n",
        "        8, 8 # <--- CRITICAL CHANGE: Spatial dimensions, derived from 64 blocks (8*8)\n",
        "        )\n",
        "# Project `K` channels to `2048` channels to match `x_bridge` (which is 16x16, 2048 channels) for fusion\n",
        "# This implies the fsm_output_fused will be (B, 2048, 8, 8).\n",
        "# This will be added to x_bridge (B, 2048, 16, 16). This creates a spatial mismatch for addition.\n",
        "# We will need to upsample fsm_output_fused to 16x16 before adding to x_bridge.\n",
        "        fsm_output_fused = self.fsm_output_fusion_transform(fsm_output_spatial_for_fusion)\n",
        "\n",
        "# NEW STEP: Upsample fsm_output_fused to match x_bridge's spatial dimensions (16x16)\n",
        "        fsm_output_fused = F.interpolate(\n",
        "        fsm_output_fused,\n",
        "        size=(x_bridge.shape[2], x_bridge.shape[3]), # Target is 16x16\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "        )\n",
        "\n",
        "# Fusion: Add FSM contribution to the bridge output (main decoder input)\n",
        "        fused_decoder_input = x_bridge + fsm_output_fused\n",
        "\n",
        "        #7.CNN Decoder: Reconstructs the mask from fused features\n",
        "        output = self.decoder(fused_decoder_input, pre_pools)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "#Dummy Data Testing\n",
        "#This block will test the model's forward pass and print its structure.\n",
        "if __name__ == '__main__':\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    print(\"Initializing TransCMFDBaseline model...\")\n",
        "    model = TransCMFDBaseline().cuda() # Instantiate model and move to GPU\n",
        "    print(\"TransCMFDBaseline model successfully loaded on cuda.\")\n",
        "\n",
        "    #Print the model summary to verify custom modules that I have used in this architecture\n",
        "    print(\"\\n--- Model Architecture Summary ---\")\n",
        "    print(model)\n",
        "    print(\"----------------------------------\\n\")\n",
        "\n",
        "    dummy_input = torch.rand((2, 3, 256, 256)).cuda() #Dummy input: Batch size 2, 3 channels (RGB), 512x512 resolution of image\n",
        "    print(f\"Dummy input shape: {dummy_input.shape}\")\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient computation for faster dummy pass (just for dummy input usage)\n",
        "        output = model(dummy_input)\n",
        "\n",
        "    print(f\"Output shape: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eHc_QIn9e6IS",
        "outputId": "265f4a2c-6352-4c28-dff6-8902837a012f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing TransCMFDBaseline model...\n",
            "TransCMFDBaseline model successfully loaded on cuda.\n",
            "\n",
            "--- Model Architecture Summary ---\n",
            "TransCMFDBaseline(\n",
            "  (encoder): Encoder(\n",
            "    (input_block): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (down_blocks): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (bridge): Bridge(\n",
            "    (bridge): Sequential(\n",
            "      (0): ConvBlock(\n",
            "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): ConvBlock(\n",
            "        (conv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 2048, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (adaptive_transformer_encoder): AdaptiveTransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x AdaptiveTransformerLayer(\n",
            "        (adaptive_mhsa): AdaptiveMultiHeadSelfAttention(\n",
            "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): Sequential(\n",
            "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (proj_to_dim): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (transformer_output_to_fsm_input): Upsample(size=(256, 256), mode='bilinear')\n",
            "  (fsm): FeatureSimilarityModule()\n",
            "  (fsm_output_fusion_transform): Conv2d(32, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (decoder): Decoder(\n",
            "    (up_block1): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(3072, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 1024, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (up_block2): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (up_block3): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (up_block4): UpBlockForUNetWithResNet50(\n",
            "      (upsample_layer): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "      (conv_block): ConvBlock(\n",
            "        (conv): Conv2d(320, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (gn): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (last_upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
            "    (last_conv): ConvBlock(\n",
            "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (gn): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (out): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            ")\n",
            "----------------------------------\n",
            "\n",
            "Dummy input shape: torch.Size([2, 3, 256, 256])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-2482798764.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Disable gradient computation for faster dummy pass (just for dummy input usage)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Output shape: {output.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-2482798764.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;31m# 3.Adaptive Transformer Encoder: Learns global representations from tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mtransformer_output_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_transformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output_c4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Output: (B, 256, dim=512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;31m# 4.Transform Transformer Output to FSM Input (\"image-like feature F\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-2482798764.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feature_map_c4)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-2482798764.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m#x is expected as token sequence: (batch_size, sequence_length, embedding_dimension)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_mhsa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Custom MHSA call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Add residual connection and apply LayerNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-2482798764.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;31m# --- DEBUGGING LINES START ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEBUG: NaN found in attention_weights before softmax!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;31m# You might want to print mean, max, min for Q, K, attention_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "# import cv2 # Not strictly needed for this version, but good to have\n",
        "\n",
        "class CMFDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Copy-Move Forgery Detection using CASIA v2.0.\n",
        "    Loads (Tampered Image, Ground Truth Mask) pairs and applies transformations.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_root, transform=None, img_size=(512, 512)):\n",
        "        self.data_root = data_root\n",
        "\n",
        "        self.tampered_folder = os.path.join(data_root, 'Tp') # Changed from 'Tampered' to 'Tp'\n",
        "        self.gt_folder = os.path.join(data_root, 'CASIA 2 Groundtruth') # Changed from 'Groundtruth' to 'CASIA 2 Groundtruth'\n",
        "\n",
        "        self.transform = transform\n",
        "        self.img_size = img_size\n",
        "\n",
        "        self.data_pairs = []\n",
        "\n",
        "        tampered_image_files = sorted([f for f in os.listdir(self.tampered_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "\n",
        "        for img_name in tampered_image_files:\n",
        "            img_path = os.path.join(self.tampered_folder, img_name)\n",
        "\n",
        "            # This part assumes mask names like 'Tp_D_CNN_S_N_txt00043_txt00051_10378_gt.png' from 'Tp_D_CNN_S_N_txt00043_txt00051_10378.jpg'\n",
        "            # Based on your previous uploads, this naming convention should be correct.\n",
        "            base_name_without_ext = os.path.splitext(img_name)[0]\n",
        "            mask_name = f\"{base_name_without_ext}_gt.png\"\n",
        "            mask_path = os.path.join(self.gt_folder, mask_name)\n",
        "\n",
        "            if os.path.exists(mask_path):\n",
        "                self.data_pairs.append((img_path, mask_path))\n",
        "            else:\n",
        "                print(f\"Warning: Mask not found for {img_name} at {mask_path}. Skipping.\")\n",
        "\n",
        "        print(f\"Loaded {len(self.data_pairs)} valid tampered image-mask pairs from {data_root}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.data_pairs[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        mask = Image.open(mask_path).convert(\"L\")\n",
        "\n",
        "        image = image.resize(self.img_size)\n",
        "        mask = mask.resize(self.img_size, Image.NEAREST)\n",
        "\n",
        "        image_tensor = transforms.ToTensor()(image)\n",
        "        mask_tensor = transforms.ToTensor()(mask)\n",
        "        mask_tensor = (mask_tensor > 0.5).float()\n",
        "\n",
        "        if self.transform:\n",
        "            image_tensor = self.transform(image_tensor)\n",
        "\n",
        "        return image_tensor, mask_tensor"
      ],
      "metadata": {
        "id": "tiNXmAAJ8o__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Import torch here\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "# import cv2 # Not strictly needed for this version, but good to have\n",
        "\n",
        "# Define transformations (ImageNet normalization is standard for ResNet pretrained)\n",
        "transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "CASIA_V2_ROOT = '/content/drive/MyDrive/TransCMFD_dataset/CASIA2'\n",
        "\n",
        "# Create the dataset instance\n",
        "full_dataset = CMFDataset(\n",
        "    data_root=CASIA_V2_ROOT,\n",
        "    transform=transform,\n",
        "    img_size=(256, 256)\n",
        ")\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "# If CASIA v2.0 has ~5123 images, this split is reasonable.\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Dataset split: Training {len(train_dataset)} samples, Validation {len(val_dataset)} samples\")\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 8 # Keep small initially, adjust based on GPU memory\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2) # num_workers > 0 for faster loading\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train DataLoader batches: {len(train_loader)}\")\n",
        "print(f\"Validation DataLoader batches: {len(val_loader)}\")\n",
        "\n",
        "# --- Test Data Loading ---\n",
        "print(\"\\nTesting data loading...\")\n",
        "try:\n",
        "    # Fetch one batch to verify shapes and types\n",
        "    images, masks = next(iter(train_loader)) # Use next(iter()) to get one batch\n",
        "    print(f\"Batch images shape: {images.shape}\") # Should be (BATCH_SIZE, 3, 512, 512)\n",
        "    print(f\"Batch images dtype: {images.dtype}\") # Should be torch.float32\n",
        "    print(f\"Batch masks shape: {masks.shape}\")   # Should be (BATCH_SIZE, 1, 512, 512)\n",
        "    print(f\"Batch masks dtype: {masks.dtype}\")   # Should be torch.float32\n",
        "    print(\"Data loading successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during data loading test: {e}\")\n",
        "    print(\"Please double-check your CASIA_V2_ROOT path, folder structure (Tampered, Groundtruth), and mask naming.\")\n",
        "    print(\"Common issues: wrong root path, incorrect subfolder names, mask files not found for image files.\")"
      ],
      "metadata": {
        "id": "4kqilib75fqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "CASIA_V2_ROOT = '/content/drive/MyDrive/TransCMFD_dataset/CASIA2'\n",
        "\n",
        "print(f\"Contents of {CASIA_V2_ROOT}:\")\n",
        "try:\n",
        "    for item in os.listdir(CASIA_V2_ROOT):\n",
        "        item_path = os.path.join(CASIA_V2_ROOT, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"  [DIR] {item}\")\n",
        "        else:\n",
        "            print(f\"  [FILE] {item}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {CASIA_V2_ROOT} not found. Please double-check the path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "2TbJN_T68fhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "CASIA_V2_ROOT = '/content/drive/MyDrive/TransCMFD_dataset/CASIA2'\n",
        "TAMPERED_FOLDER = os.path.join(CASIA_V2_ROOT, 'Tp')\n",
        "GT_FOLDER = os.path.join(CASIA_V2_ROOT, 'CASIA 2 Groundtruth')\n",
        "\n",
        "print(f\"--- Listing Tampered Images in '{TAMPERED_FOLDER}' ---\")\n",
        "try:\n",
        "    tampered_files = sorted([f for f in os.listdir(TAMPERED_FOLDER) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "    if tampered_files:\n",
        "        print(f\"Found {len(tampered_files)} tampered images. First 5:\")\n",
        "        for i, fname in enumerate(tampered_files[:5]):\n",
        "            print(f\"  {fname}\")\n",
        "\n",
        "        # Now, let's try to derive a mask name for the first tampered image\n",
        "        if tampered_files:\n",
        "            sample_img_name = tampered_files[0]\n",
        "            base_name_without_ext = os.path.splitext(sample_img_name)[0]\n",
        "            derived_mask_name = f\"{base_name_without_ext}_gt.png\" # This is the current assumption\n",
        "            expected_mask_path = os.path.join(GT_FOLDER, derived_mask_name)\n",
        "\n",
        "            print(f\"\\n--- Checking Mask for Sample Image: '{sample_img_name}' ---\")\n",
        "            print(f\"  Derived mask name: '{derived_mask_name}'\")\n",
        "            print(f\"  Expected mask path: '{expected_mask_path}'\")\n",
        "\n",
        "            if os.path.exists(expected_mask_path):\n",
        "                print(\"  --> Mask file EXISTS at the derived path! This is good.\")\n",
        "            else:\n",
        "                print(\"  --> Mask file DOES NOT EXIST at the derived path! This is the problem.\")\n",
        "                print(\"  Please check the exact naming convention of your mask files in the 'CASIA 2 Groundtruth' folder.\")\n",
        "                print(\"  Look for a mask file that corresponds to this image:\")\n",
        "                print(f\"    Image: {sample_img_name}\")\n",
        "                print(\"  And provide its exact mask filename.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"No image files found in '{TAMPERED_FOLDER}'. Please check the path and content.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: '{TAMPERED_FOLDER}' not found. Check subfolder name or root path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "axAsCyvv9RNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "import os # Ensure os is imported for checkpoint_path directory creation\n",
        "\n",
        "# Ensure loss classes are defined here or in a prior cell\n",
        "class DiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "    def forward(self, prediction, target):\n",
        "        target = target.float()\n",
        "        prediction = torch.sigmoid(prediction)\n",
        "        prediction_flat = prediction.contiguous().view(-1)\n",
        "        target_flat = target.contiguous().view(-1)\n",
        "        intersection = (prediction_flat * target_flat).sum()\n",
        "        dice_coefficient = (2. * intersection + self.smooth) / (prediction_flat.sum() + target_flat.sum() + self.smooth)\n",
        "        return 1 - dice_coefficient\n",
        "\n",
        "class AdaptiveRegularizationLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdaptiveRegularizationLoss, self).__init__()\n",
        "    def forward(self, model):\n",
        "        l_adapt_total = 0.0\n",
        "        if hasattr(model, 'adaptive_transformer_encoder') and hasattr(model.adaptive_transformer_encoder, 'layers'):\n",
        "            for layer in model.adaptive_transformer_encoder.layers:\n",
        "                if hasattr(layer, 'adaptive_mhsa'):\n",
        "                    s1_param = layer.adaptive_mhsa.S1\n",
        "                    s2_param = layer.adaptive_mhsa.S2\n",
        "                    l_adapt_total += torch.norm(s1_param, 2)**2\n",
        "                    l_adapt_total += torch.norm(s2_param, 2)**2\n",
        "        return l_adapt_total\n",
        "\n",
        "# --- Initialize Model, Loss Functions, and Optimizer ---\n",
        "# Assuming TransCMFDBaseline model, train_loader, val_loader are defined from prior cells\n",
        "\n",
        "# Define Loss Functions\n",
        "dice_loss_fn = DiceLoss(smooth=1.0)\n",
        "bce_loss_fn = nn.BCEWithLogitsLoss()\n",
        "adaptive_reg_loss_fn = AdaptiveRegularizationLoss()\n",
        "\n",
        "# Define Optimizer\n",
        "LEARNING_RATE = 1e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Loss weighting factors\n",
        "ALPHA = 0.5\n",
        "BETA = 0.5\n",
        "GAMMA = 0.01\n",
        "\n",
        "# --- Training Loop ---\n",
        "\n",
        "NUM_EPOCHS = 10 # Adjust as needed\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, dice_loss, bce_loss, adaptive_reg_loss, num_epochs, alpha, beta, gamma):\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Initialize GradScaler for Automatic Mixed Precision (AMP)\n",
        "    scaler = torch.cuda.amp.GradScaler() #\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Clear CUDA cache before training loop to free up any residual memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # --- Training Phase ---\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_loop = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")\n",
        "\n",
        "        for batch_idx, (images, masks) in enumerate(train_loop):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Use autocast for mixed precision training\n",
        "            with torch.cuda.amp.autocast():\n",
        "                predictions = model(images)\n",
        "                l_dice = dice_loss(predictions, masks)\n",
        "                l_bce = bce_loss(predictions, masks)\n",
        "                l_adapt = adaptive_reg_loss(model)\n",
        "\n",
        "                total_loss = alpha * l_dice + beta * l_bce + gamma * l_adapt\n",
        "\n",
        "            # Scale the loss and perform backward pass\n",
        "            scaler.scale(total_loss).backward()\n",
        "\n",
        "            # Optimizer step\n",
        "            scaler.step(optimizer)\n",
        "\n",
        "            # Update the scaler for next iteration\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += total_loss.item() * images.size(0)\n",
        "\n",
        "            train_loop.set_postfix(loss=total_loss.item())\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        # --- Validation Phase ---\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_loop = tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\")\n",
        "            for images, masks in val_loop:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "\n",
        "                # Use autocast for validation too (no scaler.scale/update needed)\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    predictions = model(images)\n",
        "                    l_dice_val = dice_loss(predictions, masks)\n",
        "                    l_bce_val = bce_loss(predictions, masks)\n",
        "                    l_adapt_val = adaptive_reg_loss(model) # Still calculate for total val loss\n",
        "                    total_val_loss = alpha * l_dice_val + beta * l_bce_val + gamma * l_adapt_val\n",
        "\n",
        "                val_loss += total_val_loss.item() * images.size(0)\n",
        "                val_loop.set_postfix(val_loss=total_val_loss.item())\n",
        "\n",
        "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1} Validation Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            checkpoint_path = '/content/drive/MyDrive/TransCMFD_Checkpoints/best_model.pth'\n",
        "            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)\n",
        "            torch.save(model.state_dict(), checkpoint_path)\n",
        "            print(f\"Model saved to {checkpoint_path} with validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "\n",
        "# --- Run the Training ---\n",
        "if __name__ == '__main__':\n",
        "    # Make sure to set these according to your previous successful DataLoaders setup!\n",
        "    # And your model should be instantiated and moved to CUDA in a prior cell.\n",
        "\n",
        "    # CRITICAL: These are the values you MUST adjust\n",
        "    BATCH_SIZE = 1 # <--- REDUCED BATCH SIZE\n",
        "    NUM_WORKERS = 0 # <--- REDUCED NUM_WORKERS (Set to 0 for initial debugging to avoid multiprocessing issues)\n",
        "\n",
        "    print(f\"Using BATCH_SIZE: {BATCH_SIZE}, NUM_WORKERS: {NUM_WORKERS}\")\n",
        "\n",
        "    # Re-create DataLoaders with the adjusted batch size and num_workers\n",
        "    # Ensure CASIA_V2_ROOT, transform, etc., are defined in a prior cell and correct.\n",
        "    # (Assuming these vars are available from your successful data loading setup)\n",
        "    # Be cautious about running this part multiple times if you only want to create loaders once per session.\n",
        "    try:\n",
        "        # Assuming full_dataset is already defined from a previous cell.\n",
        "        # If not, you'd need to re-instantiate CMFDataset here.\n",
        "        # For simplicity, if this is run in a separate cell, make sure full_dataset is global or passed.\n",
        "\n",
        "        # Re-split dataset if you change full_dataset (e.g., if you only partially loaded CASIA)\n",
        "        train_size = int(0.8 * len(full_dataset))\n",
        "        val_size = len(full_dataset) - train_size\n",
        "        train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "        print(\"DataLoaders re-created with new batch size and num_workers.\")\n",
        "    except NameError:\n",
        "        print(\"Warning: full_dataset not found. Ensure CMFDataset and DataLoader setup cells were run correctly.\")\n",
        "        print(\"Please define/re-run your data loading setup to ensure train_loader and val_loader are available.\")\n",
        "        # You'll need to manually ensure train_loader and val_loader are defined if this is the first execution.\n",
        "\n",
        "\n",
        "    print(\"\\nStarting model training...\")\n",
        "    train_model(model, train_loader, val_loader, optimizer, dice_loss_fn, bce_loss_fn, adaptive_reg_loss_fn, NUM_EPOCHS, ALPHA, BETA, GAMMA)"
      ],
      "metadata": {
        "id": "BCZ9By7f9Kyv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}